{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ec8d42",
   "metadata": {},
   "source": [
    "# TRANSFORMER\n",
    "\n",
    "The Transformer was proposed by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin in their paper from 2017:  \n",
    "<i>[1] \"Attention is all you need\".</i>\n",
    "\n",
    "This is, without a doubt, one of the most impactful papers of 2017. The paper presents a lot of improvements to the soft attention mechanism and makes it possible to do <i>seq2seq</i> without recurrence. The proposed \"transformer\" model is entirely built on the self-attention mechanism without using sequence-aligned recurrent architecture.\n",
    "\n",
    "![Intro](img/transformer_intro.png \"Intro\")\n",
    "\n",
    "Before developing the transformer model we need to develop a few additional fundamental components:\n",
    "* Self-attention\n",
    "* Masked decoding\n",
    "* Multi-headed self-attention\n",
    "* Adding nonlinearities\n",
    "* Positional encoding\n",
    "* Cross-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4217985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.utils.gradient_check import eval_numerical_gradient, rel_error\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071ac3fe",
   "metadata": {},
   "source": [
    "## ATTENTION AND SELF-ATTENTION\n",
    "\n",
    "Attention in deep learning can broadly be interpreted as a vector of importance weights.  \n",
    "In a sequence-to-sequence model a score $ \\alpha_{t,i} $ is assigned to the pair of output at position <i>t</i> ($ y_{t} $) and input at position <i>i</i> ($ x_{i} $).  \n",
    "For example:  \n",
    "\n",
    "$$ \\beta_{t,i} = y_{t}^{T}x_{i} $$  \n",
    "$$ \\alpha_{t,i} = \\frac{e^{\\beta_{t,i}}}{\\displaystyle\\sum_{i}{e^{\\beta_{t,i}}}} $$\n",
    "\n",
    "\n",
    "The attention vector $ \\alpha_{t} $ defines how much of each source hidden state should be considered for the output at position <i>t</i>  \n",
    "We compute the attention output as a weighted sum over the source hidden states using the vector $ \\alpha $:\n",
    "\n",
    "$$ \\displaystyle c_{t} = \\sum_{i} a_{t, i} x_{i} $$\n",
    "\n",
    "This attention output is then used to generate the next output.\n",
    "\n",
    "![Attention](img/transformer_attention.png \"Attention\")\n",
    "\n",
    "Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence.\n",
    "\n",
    "![Self Attention](img/transformer_selfattention.png \"Self Attention\")\n",
    "\n",
    "Generally, attention can be described as operating on <i>keys</i> $ k_{i} $, <i>queries</i> $ q_{i} $, and <i>values</i> $ v_{i} $: the attention vector $ \\alpha_{t} $ is computed using key-query affinities, and the attention output $ c_{t} $ is calculated as a weighted sum over the values.\n",
    "$$ \\beta_{t,i} = q_{t}^{T}k_{i} $$  \n",
    "$$ \\alpha_{t,i} = \\frac{e^{\\beta_{t,i}}}{\\displaystyle\\sum_{i}{e^{\\beta_{t,i}}}} $$\n",
    "$$ \\displaystyle c_{t} = \\sum_{i} a_{t, i} v_{i} $$\n",
    "In the case of the sequence-to-sequence model we have: $ k_{i} = x_{i} $; $ q_{i} = y_{i} $; $ v_{i} = x_{i} $ - the keys and the values are equal to the source hidden states, and the queries are equal to the outputs.\n",
    "\n",
    "In self-attention, from each input $ x_{i} $ of the layer we create three vectors: $ k_{i}, q_{i}, v_{i}$. These vectors are obtained by multiplying $ x_{i} $ with three matrices $ W^{K}, W^{Q}, W^{V} $ which are trained during the training process. These matrices allow different aspects of the $ x $ vectors to be used/emphasized in each of the three roles.\n",
    "\n",
    "![KQV](img/transformer_selfattention_kqv.png \"KQV\")\n",
    "\n",
    "One final variation is to scale the vector $ \\beta_{t, i} $ just before applying the softmax. The reasoning behind this is that when the dimensionality $ d $ of the vectors (<b>keys, queries, values</b>) becomes large, the dot products tend to also become large. Because of this, inputs to the softmax can also become large, making the gradients small. For this reason we divide the vector $ \\beta_{t, i} $ by $ \\sqrt{d} $. Thus:\n",
    "\n",
    "$$ \\alpha_{t,i} = \\large\\frac{e^{\\frac{\\beta_{t,i}}{\\sqrt{d}}}}{\\displaystyle\\sum_{i}{e^{\\frac{\\beta_{t,i}}{\\sqrt{d}}}}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54962c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " [[ 0.497 -0.138  0.648  1.523]\n",
      " [-0.234 -0.234  1.579  0.767]]\n",
      "K:\n",
      " [[-0.469  0.543 -0.463]\n",
      " [-0.466  0.242 -1.913]\n",
      " [-1.725 -0.562 -1.013]\n",
      " [ 0.314 -0.908 -1.412]]\n",
      "Q:\n",
      " [[ 1.466 -0.226  0.068]\n",
      " [-1.425 -0.544  0.111]\n",
      " [-1.151  0.376 -0.601]\n",
      " [-0.292 -0.602  1.852]]\n",
      "V:\n",
      " [[-0.013 -1.058  0.823]\n",
      " [-1.221  0.209 -1.96 ]\n",
      " [-1.328  0.197  0.738]\n",
      " [ 0.171 -0.116 -0.301]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "T = 2  # number of time-steps\n",
    "D = 4  # input dimension\n",
    "M = 3  # output dimension\n",
    "\n",
    "x = np.random.randn(T, D)\n",
    "K = np.random.randn(D, M)\n",
    "Q = np.random.randn(D, M)\n",
    "V = np.random.randn(D, M)\n",
    "\n",
    "print(\"x:\\n\", x)\n",
    "print(\"K:\\n\", K)\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"V:\\n\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab077e5",
   "metadata": {},
   "source": [
    "The input to the self-attention layer is a sequence $ [x_{1}, x_{2}, ..., x_{T}] $, and the output is again a sequence of the same lentgth $ [z_{1}, z_{2}, ..., z_{T}] $. Here $ z_{t} $ is the attention output after taking the input $ x_{t} $ and attending to every other input from the sequence (including itself).\n",
    "\n",
    "$$ z_{t} = \\sum_{i} \\alpha_{t, i} v_{i} $$\n",
    "$$ z_{t} = \\alpha_{t} V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2beef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive implementation of softmax.\n",
    "def softmax(s):\n",
    "    return np.exp(s) / np.sum(np.exp(s), axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b13852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys:\n",
      " [[-0.807 -1.511 -2.773]\n",
      " [-2.264 -1.769 -2.127]]\n",
      "queries:\n",
      " [[-0.265 -0.71   2.45 ]\n",
      " [-2.051  0.312  0.431]]\n",
      "values:\n",
      " [[-0.437 -0.603  0.699]\n",
      " [-1.677  0.421  1.201]]\n",
      "\n",
      "attention scores:\n",
      " [[0.224 0.776]\n",
      " [0.137 0.863]]\n"
     ]
    }
   ],
   "source": [
    "print(\"keys:\\n\", x @ K)\n",
    "print(\"queries:\\n\", x @ Q)\n",
    "print(\"values:\\n\", x @ V)\n",
    "\n",
    "print(\"\\nattention scores:\\n\", softmax(x @ Q @ K.T @ x.T / np.sqrt(M)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b0fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention_naive(x, K, Q, V):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (T, D).\n",
    "    - K: A numpy array of shape (D, M) containing the weights for the key matrix.\n",
    "    - Q: A numpy array of shape (D, M) containing the weights for the query matrix.\n",
    "    - V: A numpy array of shape (D, M) containing the weights for the value matrix.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data of shape (T, M)\n",
    "    \"\"\"\n",
    "    M = K.shape[-1]\n",
    "\n",
    "    keys = x @ K      # matrix of shape (T, M)\n",
    "    queries = x @ Q   # matrix of shape (T, M)\n",
    "    values = x @ V    # matrix of shape (T, M)\n",
    "\n",
    "    beta = queries @ keys.T    # matrix of shape (T, T)\n",
    "    beta /= np.sqrt(M)         # scale before applying softmax\n",
    "    alpha = softmax(beta)\n",
    "    \n",
    "    out = alpha @ values\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d3cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "z shape: (2, 3)\n",
      "z:\n",
      " [[-1.399  0.191  1.089]\n",
      " [-1.507  0.28   1.132]]\n"
     ]
    }
   ],
   "source": [
    "z = self_attention_naive(x, K, Q, V)\n",
    "print(\"\\nz shape:\", z.shape)\n",
    "print(\"z:\\n\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c588763",
   "metadata": {},
   "source": [
    "Looking at the formulas we can see that we could pipeline the operations for computing the output.  \n",
    "First take the query-key dot products in one matrix multiplication: $ \\beta = xQK^{T}x^{T} $.  \n",
    "Next, softmax, and compute the weighted average with another matrix multiplication: $ z = \\text{softmax}(\\beta) \\space xV $.  \n",
    "\n",
    "![Matrix form](img/transformer_selfattention_matrix.png \"Matrix form\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eafc3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(x, K, Q, V):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (T, D).\n",
    "    - K: A numpy array of shape (D, M) containing the weights for the key matrix.\n",
    "    - Q: A numpy array of shape (D, M) containing the weights for the query matrix.\n",
    "    - V: A numpy array of shape (D, M) containing the weights for the value matrix.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data of shape (T, M)\n",
    "    \"\"\"\n",
    "    M = K.shape[-1]\n",
    "    return softmax(x @ Q @ K.T @ x.T / np.sqrt(M)) @ x @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b63ccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z shape: (2, 3)\n",
      "z:\n",
      " [[-1.399  0.191  1.089]\n",
      " [-1.507  0.28   1.132]]\n"
     ]
    }
   ],
   "source": [
    "z = self_attention(x, K, Q, V)\n",
    "print(\"z shape:\", z.shape)\n",
    "print(\"z:\\n\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302420b",
   "metadata": {},
   "source": [
    "To perform the backward pass we need to compute the derivative of the loss $\\ell$ with respect to $ x, K, Q, V $ (the loss $ \\ell $ is a single number, i.e. $ \\ell \\in \\mathbb{R} $).  \n",
    "Assume we have the upstream derivative of $\\ell$ with respect to a tensor $z$, $\\displaystyle \\frac{d\\ell}{dz}$. Then, for any tenor $y$ such that $z=f(y)$ we have that the partail derivative of $\\ell$ with respect to a single element $y_{j}$ of the tensor $y$ is:  \n",
    "$$ \\frac{\\partial \\ell}{\\partial y_{j}} = \\sum_{i} \\frac{\\partial \\ell\n",
    "}{\\partial z_{i}} \\frac{\\partial z_{i}}{\\partial y_{j}} $$\n",
    "where the summation is over all the elements of the tensor $z$.  \n",
    "\n",
    "Suppose that $C=AB$, where $A$, $B$, and $C$ are matrices, and that $\\ell=f(C)$. In this case we have that:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\frac{d\\ell}{dB} = A^{T}\\frac{d\\ell}{dC} \\\\\n",
    "& \\frac{d\\ell}{dA} = \\frac{d\\ell}{dC}B^{T} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Using these formulas we can derive the derivatives needed for the backward pass of the self-attention layer. To simplify the derivation we will use the following notation for the loss $\\ell$ with respect to a tensor $A$: $dA = \\displaystyle \\frac{d\\ell}{dA}$\n",
    "\n",
    "For the derivative of $\\ell$ with respect to $V$ we have the following:\n",
    "$$\n",
    "\\begin{align*}\n",
    "& z = (\\alpha.x)V \\\\\n",
    "& \\frac{d\\ell}{dV} = (\\alpha.x)^{T}\\frac{d\\ell}{dz} \\\\\n",
    "& dV = (\\alpha.x)^{T}dz\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For the derivative of $\\ell$ with respect to $Q$ we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "& z = \\alpha.(x.V) = \\text{softmax}(\\beta) \\space xV \\\\\n",
    "& d\\alpha = dz.(x.V)^{T} \\\\\n",
    "& d\\beta = \\text{softmax_backward}(d\\alpha) \\\\\n",
    "& \\beta = (xQ).(K^{T}x^{T}) \\\\\n",
    "& d(xQ) = d\\beta.(K^{T}x^{T})^{T} = d\\beta.xK \\\\\n",
    "& dQ = x^{T}d(xQ) \\\\\n",
    "& dQ = x^{T} d\\beta.xK \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Similarly for the derivative of $\\ell$ with respect to $K$ we have:  \n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\beta = (xQ).(K^{T}x^{T}) \\\\\n",
    "& d(K^{T}x^{T}) = (xQ)^{T}.d\\beta = Q^{T}x^{T}.d\\beta \\\\\n",
    "& d(K^{T}) = d(K^{T}x^{T}).x = Q^{T}x^{T}.d\\beta.x \\\\\n",
    "& dK = x^{T}d\\beta.xQ \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The last equation follows from the fact that $ d(K^{T}) $ is actually the matrix $\\displaystyle \\frac{d\\ell}{dK^{T}} $.  \n",
    "$$\n",
    "\\frac{d\\ell}{dK^{T}} =\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial \\ell}{\\partial K_{11}} & \\frac{\\partial \\ell}{\\partial K_{21}} & \\dots & \\frac{\\partial \\ell}{\\partial K_{M1}} \\\\\n",
    "\\frac{\\partial \\ell}{\\partial K_{12}} & \\frac{\\partial \\ell}{\\partial K_{22}} & \\dots & \\frac{\\partial \\ell}{\\partial K_{M2}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial \\ell}{\\partial K_{1D}} & \\frac{\\partial \\ell}{\\partial K_{2D}} & \\dots & \\frac{\\partial \\ell}{\\partial K_{MD}} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Now it is obvious that $ d(K^{T}) = (dK)^{T} $.\n",
    "\n",
    "Finally, for the derivative of $\\ell$ with respect to $x$ we have:  \n",
    "$$\n",
    "\\begin{align*}\n",
    "& z = \\text{softmax}(xQK^{T}x^{T})xV = \\alpha.x.V \\\\\n",
    "& \\frac{d\\ell}{dx} = \\frac{d\\ell}{dz}\\frac{dz}{dx} = \\frac{d\\ell}{dz}\\frac{dz}{d\\alpha}\\frac{d\\alpha}{dx} + \\frac{d\\ell}{dz}\\frac{dz}{d(xV)}\\frac{d(xV)}{dx} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let:\n",
    "\n",
    "$\\displaystyle dx^{(1)} = \\frac{d\\ell}{dz}\\frac{dz}{d\\alpha}\\frac{d\\alpha}{dx} = \\frac{d\\ell}{dz}\\frac{dz}{d\\alpha}\\frac{d\\alpha}{d\\beta}\\frac{d\\beta}{dx} = \\frac{d\\ell}{d\\beta}\\frac{d\\beta}{dx}$\n",
    "\n",
    "and\n",
    "\n",
    "$\\displaystyle dx^{(2)} = \\frac{d\\ell}{dz}\\frac{dz}{d(xV)}\\frac{d(xV)}{dx} = \\frac{d\\ell}{d(xV)}\\frac{d(xV)}{dx}$  \n",
    "\n",
    "For the first part of the derivative $dx^{(1)}$ we have:  \n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\beta = xQK^{T}x^{T} \\\\\n",
    "& dx^{(11)} = ((xQK^{T})^{T} d\\beta)^{T} = d\\beta^{T} xQK^{T}\\\\\n",
    "& dx^{(12)} = d\\beta(QK^{T}x^{T})^{T} = d\\beta x KQ^{T} \\\\\n",
    "& dx^{(1)} = d\\beta^{T} xQK^{T} + d\\beta x KQ^{T} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For the second part of the derivative $dx^{(2)}$ we have:  \n",
    "$$\n",
    "\\begin{align*}\n",
    "& z = \\alpha.x.V \\\\\n",
    "& d(xV) = \\alpha^{T}.dz \\\\\n",
    "& dx^{(2)} = d(xV).V^{T} = \\alpha^{T}.dz.V^{T} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Finally we have:\n",
    "\n",
    "$$ dx = \\alpha^{T}.dz.V^{T} + d\\beta.x.KQ^{T} + d\\beta^{T}.x.QK^{T} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a3652",
   "metadata": {},
   "source": [
    "<b>Proof</b> for the second part of the derivative:  \n",
    "$ \\beta = xQK^{T}x^{T} = xAx^{T} = Bx^{T} $  \n",
    "$ \\displaystyle \\beta_{mn} = \\sum_{r} b_{mr} x_{nr} = \\sum_{r} \\left( \\sum_{s} x_{mr}a_{rs} \\right) x_{nr} $  \n",
    "\n",
    "Thus $\\beta_{m,n}$ depends on the <i>m-th</i> and on the <i>n-th</i> rows of the matrix $x$.  \n",
    "We know that:  \n",
    "\n",
    "$ \\displaystyle \\frac{\\partial \\ell}{\\partial x_{ij}} = \\sum_{k,l} \\frac{\\partial \\ell\n",
    "}{\\partial \\beta_{kl}} \\frac{\\partial \\beta_{kl}}{\\partial x_{ij}} $  \n",
    "\n",
    "Since $ x_{ij} $ is in the <i>i-th</i> row, we know that only the elements from the <i>i-th</i> row and the elements from the <i>i-th</i> column of the matrix $\\beta$ will depend on $x_{ij}$. All other partial derivatives will vanish.\n",
    "\n",
    "$ \\displaystyle \\frac{\\partial \\beta_{kl}}{\\partial x_{ij}} = 0 $ - for $ k \\neq i $ or $ l \\neq i $\n",
    "\n",
    "And so we have:  \n",
    "\n",
    "$ \\displaystyle \\frac{\\partial \\ell}{\\partial x_{ij}} = \\sum_{l} \\frac{\\partial \\ell}{\\partial \\beta_{il}} \\frac{\\partial \\beta_{il}}{\\partial x_{ij}} +  \\sum_{k} \\frac{\\partial \\ell}{\\partial \\beta_{ki}} \\frac{\\partial \\beta_{ki}}{\\partial x_{ij}} $\n",
    "\n",
    "We have to be careful with the above summation and count the term $ \\displaystyle \\frac{\\partial \\ell}{\\partial \\beta_{ii}}\\frac{\\partial \\beta_{ii}}{\\partial x_{ij}} $ only once!\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\frac{\\partial \\beta_{il}}{\\partial x_{ij}} = \\frac{\\displaystyle \\partial \\sum_{r} b_{ir}x_{lr}}{\\partial x_{ij}} = \\sum_{r} x_{lr} \\frac{\\partial b_{ir}}{\\partial x_{ij}} = \\sum_{r} x_{lr} \\frac{\\displaystyle \\partial \\sum_{s} x_{is}a_{sr}}{\\partial x_{ij}} = \\sum_{r} x_{lr}a_{jr} \\space\\text{- for $l \\neq i$} \\\\\n",
    "& \\frac{\\partial \\beta_{ki}}{x_{ij}} = \\frac{\\displaystyle \\partial \\sum_{r} b_{kr}x_{ir}}{\\partial x_{ij}} = \\sum_{r} b_{kr} \\frac{\\partial x_{ir}}{\\partial x_{ij}} = b_{kj} = \\sum_{s} x_{ks}a_{sj} \\space\\text{- for $k \\neq i$} \\\\\n",
    "& \\frac{\\partial \\beta_{ii}}{\\partial x_{ij}} = \\frac{\\displaystyle \\partial \\sum_{r} b_{ir}x_{ir}}{\\partial x_{ij}} = \\sum_{r} x_{ir} \\frac{\\partial b_{ir}}{\\partial x_{ij}} + \\sum_{r} b_{ir} \\frac{\\partial x_{ir}}{\\partial x_{ij}} = \\sum_{r} x_{ir}a_{jr} + \\sum_{s} x_{is}a_{sj} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "From the above we can see that the term $ \\displaystyle \\frac{\\partial \\beta_{ii}}{\\partial x_{ij}} $ nicely completes the other two sums.  \n",
    "And so we have:  \n",
    "\n",
    "$$ \\frac{\\partial \\ell}{\\partial x_{ij}} = \\sum_{l} \\frac{\\partial \\ell}{\\partial \\beta_{il}} \\sum_{r} x_{lr}a_{jr} + \\sum_{k} \\frac{\\partial \\ell}{\\partial \\beta_{ki}} \\sum_{s} x_{ks}a_{sj} $$\n",
    "\n",
    "The product of a sum can be expressed as the inner product between two vectors. And thus the derivative with respect to the entire matrix can be written as a matrix multiplication of three matrices:\n",
    "\n",
    "$$ \\frac{d\\ell}{dx} = d\\beta.xA^{T} + d\\beta^{T}.xA $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb113f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more stable implementation of softmax.\n",
    "def softmax_forward(x):\n",
    "    shifted = x - np.max(x, axis=-1, keepdims=True)\n",
    "    out = np.exp(shifted) / np.sum(np.exp(shifted), axis=-1, keepdims=True)\n",
    "    cache = out\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def softmax_backward(dout, cache):\n",
    "    out = cache\n",
    "    N, D = out.shape\n",
    "    diag = np.expand_dims(out, axis=2) * np.expand_dims(np.identity(D), axis=0)\n",
    "    outer = np.matmul(np.expand_dims(out, axis=2), np.expand_dims(out, axis=1))\n",
    "    dx = np.matmul(np.expand_dims(dout, axis=1), diag-outer).squeeze(axis=1)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c14179eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention_forward(x, K, Q, V):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (T, D).\n",
    "    - K: A numpy array of shape (D, M) containing the weights for the key matrix.\n",
    "    - Q: A numpy array of shape (D, M) containing the weights for the query matrix.\n",
    "    - V: A numpy array of shape (D, M) containing the weights for the value matrix.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data of shape (T, M)\n",
    "    - cache: Values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    M = K.shape[-1]\n",
    "\n",
    "    beta = x @ Q @ K.T @ x.T\n",
    "    beta /= np.sqrt(M)\n",
    "    alpha, softmax_cache = softmax_forward(beta)\n",
    "    out = alpha @ x @ V\n",
    "\n",
    "    cache = (softmax_cache, alpha, x, K, Q, V)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def self_attention_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative of the self-attention output of shape (T, M)\n",
    "    - cache: A tuple of values from the forward pass.\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x, of shape (T, D).\n",
    "    - dK: Gradient with respect to K, of shape (D, M).\n",
    "    - dQ: Gradient with respect to Q, of shape (D, M).\n",
    "    - dV: Gradient with respect to V, of shape (D, M).\n",
    "    \"\"\"\n",
    "    softmax_cache, alpha, x, K, Q, V = cache\n",
    "    M = dout.shape[-1]\n",
    "\n",
    "    dV = (alpha @ x).T @ dout\n",
    "    dalpha = dout @ (x @ V).T\n",
    "    dbeta = softmax_backward(dalpha, softmax_cache) / np.sqrt(M)\n",
    "    dQ = x.T @ dbeta @ x @ K\n",
    "    dK = x.T @ dbeta.T @ x @ Q\n",
    "    dx = alpha.T @ dout @ V.T\n",
    "    dx += dbeta @ x @ K @ Q.T + dbeta.T @ x @ Q @ K.T\n",
    "\n",
    "    return dx, dK, dQ, dV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f303ef58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z shape: (2, 3)\n",
      "z:\n",
      " [[-1.399  0.191  1.089]\n",
      " [-1.507  0.28   1.132]]\n"
     ]
    }
   ],
   "source": [
    "z, cache = self_attention_forward(x, K, Q, V)\n",
    "print(\"z shape:\", z.shape)\n",
    "print(\"z:\\n\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa0fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy loss function. Returns loss value, and gradient of the loss w.r.t. the input.\n",
    "def loss(x):\n",
    "    return np.sum(x), np.ones_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63686094",
   "metadata": {},
   "source": [
    "To check the implementation of the backward pass we will use the numeric gradient. If the implementation is correct, the difference between the numeric and the analytic gradients should be less than 1e-8 for each of the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40e38572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x max relative error: 1.453740e-10\n",
      "K max relative error: 2.955696e-09\n",
      "Q max relative error: 3.938783e-09\n",
      "V max relative error: 1.953700e-11\n"
     ]
    }
   ],
   "source": [
    "l, dz = loss(z)\n",
    "dx, dK, dQ, dV = self_attention_backward(dz, cache)\n",
    "params = {\"x\":x, \"K\":K, \"Q\":Q, \"V\":V}\n",
    "grads = {\"x\":dx, \"K\":dK, \"Q\":dQ, \"V\":dV}\n",
    "\n",
    "# These should all be less than 1e-8 or so.\n",
    "f = lambda a: loss(self_attention_forward(x, K, Q, V)[0])[0]\n",
    "for _name in grads:\n",
    "    grad_numeric = eval_numerical_gradient(f, params[_name], verbose=False)\n",
    "    print(\"%s max relative error: %e\" % (_name, rel_error(grad_numeric, grads[_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080cc26a",
   "metadata": {},
   "source": [
    "Finally, to make the self-attention layer work on a batch of input sequences we need to note that the gradient of the loss $\\ell$ with respect to a weight matrix ($K, Q, V$) will be equal to the sum of the gradients from each data-point from the batch.\n",
    "\n",
    "$$ \\frac{d\\ell}{dW} = \\sum_{i=0}^{N-1} \\frac{d\\ell}{dW^{i}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcf0d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_selfattention_forward(x, K, Q, V):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (N, T, D).\n",
    "    - K: A numpy array of shape (D, M) containing the weights for the key matrix.\n",
    "    - Q: A numpy array of shape (D, M) containing the weights for the query matrix.\n",
    "    - V: A numpy array of shape (D, M) containing the weights for the value matrix.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data of shape (N, T, M)\n",
    "    - cache: Values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    N, T, D = x.shape\n",
    "    M = K.shape[-1]\n",
    " \n",
    "    beta = x @ np.expand_dims(Q, axis=0) @ np.expand_dims(K.T, axis=0) @ x.transpose(0,2,1)\n",
    "    beta /= np.sqrt(M)\n",
    "    alpha, softmax_cache = softmax_forward(beta.reshape(N*T, -1))   # tensor (N, T, T)\n",
    "    alpha = alpha.reshape(N, T, -1)\n",
    "    out = alpha @ x @ np.expand_dims(V, axis=0)\n",
    "\n",
    "    cache = (softmax_cache, alpha, x, K, Q, V)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def batch_selfattention_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative of the self-attention output of shape (N, T, M)\n",
    "    - cache: A tuple of values from the forward pass.\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x, of shape (N, T, D).\n",
    "    - dK: Gradient with respect to K, of shape (D, M).\n",
    "    - dQ: Gradient with respect to Q, of shape (D, M).\n",
    "    - dV: Gradient with respect to V, of shape (D, M).\n",
    "    \"\"\"\n",
    "    softmax_cache, alpha, x, K, Q, V = cache\n",
    "    N, T, M = dout.shape\n",
    "\n",
    "    dV = np.sum((alpha @ x).transpose(0,2,1) @ dout, axis=0)\n",
    "    dalpha = dout @ (x @ np.expand_dims(V, axis=0)).transpose(0,2,1)\n",
    "    dbeta = softmax_backward(dalpha.reshape(N*T,-1), softmax_cache) / np.sqrt(M)\n",
    "    dbeta = dbeta.reshape(N, T, -1)\n",
    "    dQ = np.sum(x.transpose(0,2,1) @ dbeta @ x @ np.expand_dims(K, axis=0), axis=0)\n",
    "    dK = np.sum(x.transpose(0,2,1) @ dbeta.transpose(0,2,1) @ x @ np.expand_dims(Q, axis=0), axis=0)\n",
    "    dx = alpha.transpose(0,2,1) @ dout @ np.expand_dims(V.T, axis=0)    \n",
    "    dx += dbeta @ x @ np.expand_dims(K @ Q.T, axis=0)\n",
    "    dx += dbeta.transpose(0,2,1) @ x @ np.expand_dims(Q @ K.T, axis=0)\n",
    "\n",
    "    return dx, dK, dQ, dV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a28a90cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch shape: (3, 2, 4)\n",
      "z shape: (3, 2, 3)\n",
      "z:\n",
      " [[[-1.399  0.191  1.089]\n",
      "  [-1.507  0.28   1.132]]\n",
      "\n",
      " [[-1.399  0.191  1.089]\n",
      "  [-1.507  0.28   1.132]]\n",
      "\n",
      " [[-1.399  0.191  1.089]\n",
      "  [-1.507  0.28   1.132]]]\n"
     ]
    }
   ],
   "source": [
    "N = 3   # batch size\n",
    "x = np.stack([x] * N)\n",
    "print(\"x_batch shape:\", x.shape)\n",
    "z, cache = batch_selfattention_forward(x, K, Q, V)\n",
    "print(\"z shape:\", z.shape)\n",
    "print(\"z:\\n\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b4571fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x max relative error: 1.453740e-10\n",
      "K max relative error: 4.305619e-09\n",
      "Q max relative error: 3.938783e-09\n",
      "V max relative error: 2.859633e-11\n"
     ]
    }
   ],
   "source": [
    "l, dz = loss(z)\n",
    "dx, dK, dQ, dV = batch_selfattention_backward(dz, cache)\n",
    "params = {\"x\":x, \"K\":K, \"Q\":Q, \"V\":V}\n",
    "grads = {\"x\":dx, \"K\":dK, \"Q\":dQ, \"V\":dV}\n",
    "\n",
    "# These should all be less than 1e-8 or so.\n",
    "f = lambda a: loss(batch_selfattention_forward(x, K, Q, V)[0])[0]\n",
    "for _name in grads:\n",
    "    grad_numeric = eval_numerical_gradient(f, params[_name], verbose=False)\n",
    "    print(\"%s max relative error: %e\" % (_name, rel_error(grad_numeric, grads[_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266262a3",
   "metadata": {},
   "source": [
    "## MASKED DECODING\n",
    "\n",
    "To use self-attention in the decoder, we need to ensure we can't peek at the future. In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequece. This is done by masking future positions by setting attention scores to $-\\infty$.\n",
    "\n",
    "![Mask Attention](img/transformer_maskattention.png \"Mask Attention\")\n",
    "\n",
    "In practice instead of setting $e_{i,j} = -\\infty$ we just replace $\\text{exp}(e_{i,j}) = 0$.  \n",
    "However, this implementation uses the naive approach of setting $e_{i,j} = \\text{-1e-20}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e81bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_batch_selfattention_forward(x, K, Q, V, mask=False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (N, T, D).\n",
    "    - K: A numpy array of shape (D, M) containing the weights for the key matrix.\n",
    "    - Q: A numpy array of shape (D, M) containing the weights for the query matrix.\n",
    "    - V: A numpy array of shape (D, M) containing the weights for the value matrix.\n",
    "    - mask: A numpy array of shape (T, T) of boolean values. Flag=True masks the value.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data of shape (N, T, M)\n",
    "    - cache: Values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    N, T, D = x.shape\n",
    "    M = K.shape[-1]\n",
    "\n",
    "    beta = x @ np.expand_dims(Q, axis=0) @ np.expand_dims(K.T, axis=0) @ x.transpose(0,2,1)\n",
    "    beta /= np.sqrt(M)\n",
    "    beta += np.expand_dims(mask * (-1e20), axis=0)                  # tricks are for kids\n",
    "    alpha, softmax_cache = softmax_forward(beta.reshape(N*T, -1))   # tensor (N, T, T)\n",
    "    alpha = alpha.reshape(N, T, -1)\n",
    "    out = alpha @ x @ np.expand_dims(V, axis=0)\n",
    "    cache = (softmax_cache, alpha, mask, x, K, Q, V)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def masked_batch_selfattention_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative of the self-attention output of shape (N, T, M)\n",
    "    - cache: A tuple of values from the forward pass.\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x, of shape (N, T, D).\n",
    "    - dK: Gradient with respect to K, of shape (D, M).\n",
    "    - dQ: Gradient with respect to Q, of shape (D, M).\n",
    "    - dV: Gradient with respect to V, of shape (D, M).\n",
    "    \"\"\"\n",
    "    softmax_cache, alpha, mask, x, K, Q, V = cache\n",
    "    N, T, M = dout.shape\n",
    "\n",
    "    dV = np.sum((alpha @ x).transpose(0,2,1) @ dout, axis=0)\n",
    "    dalpha = dout @ (x @ np.expand_dims(V, axis=0)).transpose(0,2,1)\n",
    "    dalpha *= np.expand_dims(1-mask, axis=0)\n",
    "    dbeta = softmax_backward(dalpha.reshape(N*T,-1), softmax_cache) / np.sqrt(M)\n",
    "    dbeta = dbeta.reshape(N, T, -1)\n",
    "    dQ = np.sum(x.transpose(0,2,1) @ dbeta @ x @ np.expand_dims(K, axis=0), axis=0)\n",
    "    dK = np.sum(x.transpose(0,2,1) @ dbeta.transpose(0,2,1) @ x @ np.expand_dims(Q, axis=0), axis=0)\n",
    "    dx = alpha.transpose(0,2,1) @ dout @ np.expand_dims(V.T, axis=0)    \n",
    "    dx += dbeta @ x @ np.expand_dims(K @ Q.T, axis=0)\n",
    "    dx += dbeta.transpose(0,2,1) @ x @ np.expand_dims(Q @ K.T, axis=0)\n",
    "\n",
    "    return dx, dK, dQ, dV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1faf8b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observe that the output from the first time step z_1 attends only\n",
      "to the value at the first time step v_1\n",
      "\n",
      "z shape: (3, 2, 3)\n",
      "z:\n",
      " [[[-0.437 -0.603  0.699]\n",
      "  [-1.507  0.28   1.132]]\n",
      "\n",
      " [[-0.437 -0.603  0.699]\n",
      "  [-1.507  0.28   1.132]]\n",
      "\n",
      " [[-0.437 -0.603  0.699]\n",
      "  [-1.507  0.28   1.132]]]\n",
      "\n",
      "values:\n",
      " [[-0.437 -0.603  0.699]\n",
      " [-1.677  0.421  1.201]]\n"
     ]
    }
   ],
   "source": [
    "mask = np.array([[False, True],    # the first output will attend only to the first value vector\n",
    "                 [False, False]],  # the second output will attend to both the first and the second value vector\n",
    "                dtype=bool)\n",
    "\n",
    "print(\"\"\"Observe that the output from the first time step z_1 attends only\n",
    "to the value at the first time step v_1\\n\"\"\")\n",
    "\n",
    "z, cache = masked_batch_selfattention_forward(x, K, Q, V, mask)\n",
    "print(\"z shape:\", z.shape)\n",
    "print(\"z:\\n\", z)\n",
    "\n",
    "values = x[0] @ V\n",
    "print(\"\\nvalues:\\n\", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6174842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x max relative error: 5.350397e-11\n",
      "K max relative error: 1.066312e-08\n",
      "Q max relative error: 5.230506e-09\n",
      "V max relative error: 2.457892e-11\n"
     ]
    }
   ],
   "source": [
    "l, dz = loss(z)\n",
    "dx, dK, dQ, dV = masked_batch_selfattention_backward(dz, cache)\n",
    "params = {\"x\":x, \"K\":K, \"Q\":Q, \"V\":V}\n",
    "grads = {\"x\":dx, \"K\":dK, \"Q\":dQ, \"V\":dV}\n",
    "\n",
    "# These should all be less than 1e-8 or so.\n",
    "f = lambda a: loss(masked_batch_selfattention_forward(x, K, Q, V, mask)[0])[0]\n",
    "for _name in grads:\n",
    "    grad_numeric = eval_numerical_gradient(f, params[_name], verbose=False)\n",
    "    print(\"%s max relative error: %e\" % (_name, rel_error(grad_numeric, grads[_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb7ba1d",
   "metadata": {},
   "source": [
    "## MULTI-HEADED SELF-ATTENTION\n",
    "\n",
    "One problem with the proposed self-attention mechanism is that an output $z_{t}$ will most likely be dominated by a single $v_{i}$ because of the softmax function.\n",
    "\n",
    "$$ z_{t} = \\sum_{i} \\alpha_{t,i}v_{i} $$\n",
    "\n",
    "If we want to \"look\" at multiple places at once we could define multiple <b>attention heads</b> using multiple $K$,$Q$,$V$ matrices. Each attention head performs attention computation independantly and then the outputs from all heads are concatenated. This way each head gets to \"look\" at different things, and constructs value vectors differently.  \n",
    "This modification expands the model's ability to focus on different positions and gives the attention layer multiple representation subspaces.\n",
    "\n",
    "![Multi-head](img/transformer_multihead.png \"Multi-head\")\n",
    "\n",
    "The dimension of the output is equal to the number of attention heads ($h$) multiplied by the dimension of the output of a single head ($d$). Thus the concatenated output has dimension $dh$. Usually we want the output to have the same dimension as the input. To achieve this we simply multiply the concatenated result by a transformation matrix $W$ with dimensions $dh \\space x \\space D$, where $D$ is the dimension of the input.  \n",
    "\n",
    "![Multi-head KQV](img/transformer_multiheadKQV.png \"Multi-head KQV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07ac8c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K shape: (3, 4, 3)\n",
      "Q shape: (3, 4, 3)\n",
      "V shape: (3, 4, 3)\n",
      "\n",
      "W shape: (9, 4)\n",
      "W:\n",
      " [[-1.479 -0.72  -0.461  1.057]\n",
      " [ 0.344 -1.763  0.324 -0.385]\n",
      " [-0.677  0.612  1.031  0.931]\n",
      " [-0.839 -0.309  0.331  0.976]\n",
      " [-0.479 -0.186 -1.106 -1.196]\n",
      " [ 0.813  1.356 -0.072  1.004]\n",
      " [ 0.362 -0.645  0.361  1.538]\n",
      " [-0.036  1.565 -2.62   0.822]\n",
      " [ 0.087 -0.299  0.092 -1.988]]\n"
     ]
    }
   ],
   "source": [
    "h = 3  # number of attention heads\n",
    "K = np.stack([K] * h)\n",
    "Q = np.stack([Q] * h)\n",
    "V = np.stack([V] * h)\n",
    "\n",
    "print(\"K shape:\", K.shape)\n",
    "print(\"Q shape:\", Q.shape)\n",
    "print(\"V shape:\", V.shape)\n",
    "\n",
    "W = np.random.randn(h * M, D)\n",
    "print(\"\\nW shape:\", W.shape)\n",
    "print(\"W:\\n\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79675197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention_naive(x, K, Q, V, W, mask=False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (N, T, D).\n",
    "    - K: A numpy array of shape (h, D, M) containing the weights for the key matrix.\n",
    "    - Q: A numpy array of shape (h, D, M) containing the weights for the query matrix.\n",
    "    - V: A numpy array of shape (h, D, M) containing the weights for the value matrix.\n",
    "    - W: A numpy array of shape (hM, D) contaning the weights for the transformation matrix.\n",
    "    - mask: A numpy array of shape (T, T) of boolean values. Flag=True masks the value.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data of shape (N, T, D)\n",
    "    - cache: Values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    N, T, D = x.shape\n",
    "    h, _, M = K.shape\n",
    "    heads_out = []\n",
    "    \n",
    "    for i in range(h):\n",
    "        out, _ = masked_batch_selfattention_forward(x, K[i], Q[i], V[i], mask) # tensor of shape (N, T, M)\n",
    "        heads_out.append(out)\n",
    "    heads_out = np.dstack(heads_out)  # tensor of shape (N, T, hM)\n",
    "    print(\"heads out:\\n\", heads_out)\n",
    "    return heads_out @ np.expand_dims(W, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d315f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (3, 2, 4)\n",
      "\n",
      "Note that the output before transfromation is equal to\n",
      "the ouput from a single head concatenaded 3 times\n",
      "heads out:\n",
      " [[[-1.399  0.191  1.089 -1.399  0.191  1.089 -1.399  0.191  1.089]\n",
      "  [-1.507  0.28   1.132 -1.507  0.28   1.132 -1.507  0.28   1.132]]\n",
      "\n",
      " [[-1.399  0.191  1.089 -1.399  0.191  1.089 -1.399  0.191  1.089]\n",
      "  [-1.507  0.28   1.132 -1.507  0.28   1.132 -1.507  0.28   1.132]]\n",
      "\n",
      " [[-1.399  0.191  1.089 -1.399  0.191  1.089 -1.399  0.191  1.089]\n",
      "  [-1.507  0.28   1.132 -1.507  0.28   1.132 -1.507  0.28   1.132]]]\n",
      "\n",
      "Output after transformation:\n",
      "z shape: (3, 2, 4)\n",
      "z:\n",
      " [[[ 2.946  4.086  0.168 -5.198]\n",
      "  [ 3.152  4.305 -0.114 -5.654]]\n",
      "\n",
      " [[ 2.946  4.086  0.168 -5.198]\n",
      "  [ 3.152  4.305 -0.114 -5.654]]\n",
      "\n",
      " [[ 2.946  4.086  0.168 -5.198]\n",
      "  [ 3.152  4.305 -0.114 -5.654]]]\n",
      "\n",
      "Masked output:\n",
      "heads out:\n",
      " [[[-0.437 -0.603  0.699 -0.437 -0.603  0.699 -0.437 -0.603  0.699]\n",
      "  [-1.507  0.28   1.132 -1.507  0.28   1.132 -1.507  0.28   1.132]]\n",
      "\n",
      " [[-0.437 -0.603  0.699 -0.437 -0.603  0.699 -0.437 -0.603  0.699]\n",
      "  [-1.507  0.28   1.132 -1.507  0.28   1.132 -1.507  0.28   1.132]]\n",
      "\n",
      " [[-0.437 -0.603  0.699 -0.437 -0.603  0.699 -0.437 -0.603  0.699]\n",
      "  [-1.507  0.28   1.132 -1.507  0.28   1.132 -1.507  0.28   1.132]]]\n",
      "z:\n",
      " [[[ 1.114  2.13   2.684 -1.14 ]\n",
      "  [ 3.152  4.305 -0.114 -5.654]]\n",
      "\n",
      " [[ 1.114  2.13   2.684 -1.14 ]\n",
      "  [ 3.152  4.305 -0.114 -5.654]]\n",
      "\n",
      " [[ 1.114  2.13   2.684 -1.14 ]\n",
      "  [ 3.152  4.305 -0.114 -5.654]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"x shape:\", x.shape)\n",
    "\n",
    "print(\"\"\"\\nNote that the output before transfromation is equal to\n",
    "the ouput from a single head concatenaded {:d} times\"\"\".format(h))\n",
    "\n",
    "z = multihead_attention_naive(x, K, Q, V, W)\n",
    "print(\"\\nOutput after transformation:\")\n",
    "print(\"z shape:\", z.shape)\n",
    "print(\"z:\\n\", z)\n",
    "\n",
    "print(\"\\nMasked output:\")\n",
    "z = multihead_attention_naive(x, K, Q, V, W, mask)\n",
    "print(\"z:\\n\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907919c1",
   "metadata": {},
   "source": [
    "Finally, to make the multi-head self-attention layer work using only matrix algebra we need to note that the gradient of the loss $\\ell$ with respect to the input tensor (with shape $(N, T, D)$) will be equal to the sum of the gradients from each attention head.\n",
    "\n",
    "$$ \\frac{d\\ell}{dx} = \\sum_{i=0}^{h-1} \\frac{d\\ell}{dx^{i}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eec3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b=0):\n",
    "    _x = x.reshape(x.shape[0], -1)\n",
    "    out = np.dot(_x, w) + b\n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "    x, w, b = cache\n",
    "    _x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "    db = np.sum(dout, axis=0)\n",
    "    dw = np.dot(_x.T, dout)\n",
    "    dx = np.dot(dout, w.T)\n",
    "    dx = dx.reshape(x.shape)\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9edd1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_selfattention_forward(x, K, Q, V, W, mask=False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (N, T, D).\n",
    "    - K: A numpy array of shape (h, D, M) containing the weights for the key matrix.\n",
    "    - Q: A numpy array of shape (h, D, M) containing the weights for the query matrix.\n",
    "    - V: A numpy array of shape (h, D, M) containing the weights for the value matrix.\n",
    "    - W: A numpy array of shape (hM, D) contaning the weights for the transformation matrix.\n",
    "    - mask: A numpy array of shape (T, T) of boolean values.  Flag=True masks the value.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data of shape (N, T, D)\n",
    "    - cache: Values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    N, T, D = x.shape\n",
    "    h, _, M = K.shape\n",
    "\n",
    "    beta = np.expand_dims(x, axis=1) @ np.expand_dims(Q, axis=0) \\\n",
    "         @ np.expand_dims(K.transpose(0,2,1), axis=0) @ np.expand_dims(x.transpose(0,2,1), axis=1)\n",
    "    beta /= np.sqrt(M)\n",
    "    beta += np.expand_dims(mask * (-1e20), axis=(0,1))              # tricks are for kids\n",
    "    alpha, softmax_cache = softmax_forward(beta.reshape(N*h*T, -1))\n",
    "    alpha = alpha.reshape(N, h, T, T)\n",
    "    heads_out = alpha @ np.expand_dims(x, axis=1) @ np.expand_dims(V, axis=0)\n",
    "    heads_out = heads_out.transpose(0, 2, 1, 3).reshape(N, T, h*M)\n",
    "    out, affine_cache = affine_forward(heads_out.reshape(N*T, -1), W)\n",
    "    out = out.reshape(N, T, D)\n",
    "\n",
    "    cache = (softmax_cache, affine_cache, alpha, x, K, Q, V, mask)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def multihead_selfattention_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative of the self-attention output of shape (N, T, D)\n",
    "    - cache: A tuple of values from the forward pass.\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x, of shape (N, T, D).\n",
    "    - dK: Gradient with respect to K, of shape (h, D, M).\n",
    "    - dQ: Gradient with respect to Q, of shape (h, D, M).\n",
    "    - dV: Gradient with respect to V, of shape (h, D, M).\n",
    "    \"\"\"\n",
    "    softmax_cache, affine_cache, alpha, x, K, Q, V, mask = cache\n",
    "    N, T, D = dout.shape\n",
    "    h, _, M = K.shape\n",
    "\n",
    "    dheads_out, dW, _ = affine_backward(dout.reshape(N*T, -1), affine_cache)\n",
    "    dheads_out = dheads_out.reshape(N, T, h, M).transpose(0, 2, 1, 3)\n",
    "\n",
    "    dV = np.sum((alpha @ np.expand_dims(x, axis=1)).transpose(0,1,3,2) @ dheads_out, axis=0)\n",
    "    dalpha = dheads_out @ (np.expand_dims(x, axis=1) @ np.expand_dims(V, axis=0)).transpose(0,1,3,2)\n",
    "    dalpha *= np.expand_dims(1-mask, axis=(0,1))\n",
    "    dbeta = softmax_backward(dalpha.reshape(N*h*T,-1), softmax_cache) / np.sqrt(M)\n",
    "    dbeta = dbeta.reshape(N, h, T, T)\n",
    "    dQ = np.sum(np.expand_dims(x.transpose(0,2,1), axis=1) @ dbeta @ np.expand_dims(x, axis=1) \\\n",
    "        @ np.expand_dims(K, axis=0), axis=0)\n",
    "    dK = np.sum(np.expand_dims(x.transpose(0,2,1), axis=1) @ dbeta.transpose(0,1,3,2) \\\n",
    "        @ np.expand_dims(x, axis=1) @ np.expand_dims(Q, axis=0), axis=0)\n",
    "    dx = alpha.transpose(0,1,3,2) @ dheads_out @ np.expand_dims(V.transpose(0,2,1), axis=0)\n",
    "    dx += dbeta @ np.expand_dims(x, axis=1) @ np.expand_dims(K @ Q.transpose(0,2,1), axis=0)\n",
    "    dx += dbeta.transpose(0,1,3,2) @ np.expand_dims(x, axis=1) @ np.expand_dims(Q @ K.transpose(0,2,1), axis=0)\n",
    "    dx = np.sum(dx, axis=1)\n",
    "\n",
    "    return dx, dK, dQ, dV, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba55b780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z shape: (3, 2, 4)\n",
      "z:\n",
      " [[[ 2.946  4.086  0.168 -5.198]\n",
      "  [ 3.152  4.305 -0.114 -5.654]]\n",
      "\n",
      " [[ 2.946  4.086  0.168 -5.198]\n",
      "  [ 3.152  4.305 -0.114 -5.654]]\n",
      "\n",
      " [[ 2.946  4.086  0.168 -5.198]\n",
      "  [ 3.152  4.305 -0.114 -5.654]]]\n"
     ]
    }
   ],
   "source": [
    "z, cache = multihead_selfattention_forward(x, K, Q, V, W)\n",
    "print(\"z shape:\", z.shape)\n",
    "print(\"z:\\n\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56a3d96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x max relative error: 4.381519e-09\n",
      "K max relative error: 3.147993e-09\n",
      "Q max relative error: 5.043319e-09\n",
      "V max relative error: 1.714666e-09\n"
     ]
    }
   ],
   "source": [
    "l, dz = loss(z)\n",
    "dx, dK, dQ, dV, dW = multihead_selfattention_backward(dz, cache)\n",
    "params = {\"x\":x, \"K\":K, \"Q\":Q, \"V\":V}\n",
    "grads = {\"x\":dx, \"K\":dK, \"Q\":dQ, \"V\":dV}\n",
    "\n",
    "# These should all be less than 1e-8 or so.\n",
    "f = lambda a: loss(multihead_selfattention_forward(x, K, Q, V, W)[0])[0]\n",
    "for _name in grads:\n",
    "    grad_numeric = eval_numerical_gradient(f, params[_name], verbose=False)\n",
    "    print(\"%s max relative error: %e\" % (_name, rel_error(grad_numeric, grads[_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38194ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Masked output:\n",
      "z:\n",
      " [[[ 1.114  2.13   2.684 -1.14 ]\n",
      "  [ 3.152  4.305 -0.114 -5.654]]\n",
      "\n",
      " [[ 1.114  2.13   2.684 -1.14 ]\n",
      "  [ 3.152  4.305 -0.114 -5.654]]\n",
      "\n",
      " [[ 1.114  2.13   2.684 -1.14 ]\n",
      "  [ 3.152  4.305 -0.114 -5.654]]]\n"
     ]
    }
   ],
   "source": [
    "z, cache = multihead_selfattention_forward(x, K, Q, V, W, mask)\n",
    "print(\"\\nMasked output:\")\n",
    "print(\"z:\\n\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6130d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x max relative error: 7.402705e-10\n",
      "K max relative error: 8.872991e-09\n",
      "Q max relative error: 7.414079e-09\n",
      "V max relative error: 5.805844e-10\n"
     ]
    }
   ],
   "source": [
    "l, dz = loss(z)\n",
    "dx, dK, dQ, dV, dW = multihead_selfattention_backward(dz, cache)\n",
    "params = {\"x\":x, \"K\":K, \"Q\":Q, \"V\":V}\n",
    "grads = {\"x\":dx, \"K\":dK, \"Q\":dQ, \"V\":dV}\n",
    "\n",
    "# These should all be less than 1e-8 or so.\n",
    "f = lambda a: loss(multihead_selfattention_forward(x, K, Q, V, W, mask)[0])[0]\n",
    "for _name in grads:\n",
    "    grad_numeric = eval_numerical_gradient(f, params[_name], verbose=False)\n",
    "    print(\"%s max relative error: %e\" % (_name, rel_error(grad_numeric, grads[_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c93cc",
   "metadata": {},
   "source": [
    "## ADDING NON-LINEARITIES\n",
    "\n",
    "So far every attention layer is a linear transformation of the previous layer (with non-linear weights on the value vectors). There are no elementwise non linearities. Thus, stacking more self-attention layers just re-averages the value vectors.  \n",
    "To fix this we add a feed-forward network to post process each output of the self-attention layer.\n",
    "\n",
    "![Non-linearity](img/transformer_nonlinearity.png \"Non-linearity\")\n",
    "\n",
    "One additional detail in the architecture is that every layer has a residual connection and is followed by a layer-normalization step. Both residual connections and layer normalization are only used to help the models train better.  \n",
    "Residual connections are thought to make the loss landscape considerably smoothe. And layer normalization is thought to cut down on the uninformative variation of the input vectors.\n",
    "\n",
    "![Residuals](img/transformer_residuals.png \"Residuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5938ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_forward(x, y):\n",
    "    return x + y, None\n",
    "\n",
    "def residual_backward(dout, cache):\n",
    "    dx = dout\n",
    "    dy = dout\n",
    "    return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d74c8",
   "metadata": {},
   "source": [
    "## POSITIONAL ENCODING\n",
    "\n",
    "Since the self-attention layer doesn't account for the order of the input sequence, we need to encode the order in some way. Naively we could just try to append the sequence number to the input:\n",
    "\n",
    "$$ \\bar{x_{t}} = \\begin{bmatrix} x_{t} \\\\ t \\end{bmatrix}$$\n",
    "\n",
    "However this approach is not a good idea, because <b>absolute</b> position is less important than <b>relative</b> position. We want to represent position in a way that tokens with similar relative position have similar positional encoding.\n",
    "\n",
    "<b>One idea</b> is to use a <b>frequency-based</b> representation: concatenate sinusoidal functions of varaying periods. What this means is that basically for every input we add an indicator whether input comes from the first or the second half of the sequence. And an indicator from which quarter the input comes, and so on. And finally, whether the input has an even or and odd index.  \n",
    "\n",
    "<b>Another idea</b> is to let all $p_{i}$ be learnable parameters and learn a matrix $p \\in \\mathbb{R}^{dxT}$. This approach is perhaps more optimal than sin/cos encoding, but the downside is that we need to pick the maximum sequence lenght and cannot generalize beyond it. Most systems use this approach.\n",
    "\n",
    "To incorporate the positional encoding we could just concatenate it to the input:\n",
    "\n",
    "$$ \\bar{x_{t}} = \\begin{bmatrix} x_{t} \\\\ p_{t} \\end{bmatrix} $$\n",
    "\n",
    "Another approach is to add the positional encoding to the input. And yet another approach is to add the positional encoding to the key, query and value of the input.\n",
    "\n",
    "$ \\bar{x_{t}} = x_{t} + p_{t} $ (most people do this)  \n",
    "\n",
    "or  \n",
    "\n",
    "$ \\bar{k_{t}} = k_{t} + p_{t} $  \n",
    "$ \\bar{q_{t}} = q_{t} + p_{t} $  \n",
    "$ \\bar{v_{t}} = v_{t} + p_{t} $  \n",
    "\n",
    "![Position](img/transformer_position.png \"Position\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f02244",
   "metadata": {},
   "source": [
    "## CROSS-ATTENTION\n",
    "\n",
    "Self-attention is when keys, queries, and values come from the same source. However, we want the decoder to attend to the input sequence. To do this we use the output of the top encoder and transform it into a set of keys and queries. These are to be used by each decoder in the cross-attention layer.  \n",
    "Let $ y = [y_{1}, ..., y_{T}]$ be the output from the top encoder; $y \\in \\mathbb{R}^{Txd}$.  \n",
    "Let $ x = [x_{1}, ..., x_{T}]$ be the input to the decoder; $x \\in \\mathbb{R}^{Txd}$.  \n",
    "Then the keys and values are drawn from the encoder (like a memory):\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\text{keys} = yQ \\\\\n",
    "& \\text{values} = yV \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And the queries are drawn from the decoder:\n",
    "\n",
    "$$ \\text{queries} = xQ $$\n",
    "\n",
    "The cross-attention output is computed similarly to the self-attention output:\n",
    "\n",
    "$$ \\text{out} = \\text{softmax}(xQK^{T}y^{T})yV $$\n",
    "\n",
    "![Cross attention](img/transformer_crossattention_matrix.png \"Cross attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a17f710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_attention_naive(x, y, K, Q, V):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (Tdec, D), giving decoder inputs.\n",
    "    - y: A numpy array of shape (Tenc, D), giving encoder outputs.\n",
    "    - K: A numpy array of shape (D, M) containing the weights for the key matrix.\n",
    "    - Q: A numpy array of shape (D, M) containing the weights for the query matrix.\n",
    "    - V: A numpy array of shape (D, M) containing the weights for the value matrix.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data of shape (Tdec, M)\n",
    "    - cache: Values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    M = K.shape[-1]\n",
    "    return softmax(x @ Q @ K.T @ y.T / np.sqrt(M)) @ y @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c13436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (2, 4)\n",
      "z shape: (4, 3)\n",
      "z:\n",
      " [[-1.699  0.752  0.58 ]\n",
      " [-2.284  0.682  0.421]\n",
      " [-1.566  0.768  0.616]\n",
      " [-2.338  0.676  0.407]]\n"
     ]
    }
   ],
   "source": [
    "Tenc = T\n",
    "Tdec = 4\n",
    "\n",
    "y = np.random.randn(Tenc, D)\n",
    "x = np.random.randn(Tdec, D)\n",
    "z = cross_attention_naive(x, y, K[0], Q[0], V[0])\n",
    "\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"z shape:\", z.shape)\n",
    "print(\"z:\\n\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6cb16ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_cross_attention_forward(x, y, K, Q, V):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (N, Tdec, D), giving decoder inputs.\n",
    "    - y: A numpy array of shape (N, Tenc, D), giving encoder outputs.\n",
    "    - K: A numpy array of shape (D, M) containing the weights for the key matrix.\n",
    "    - Q: A numpy array of shape (D, M) containing the weights for the query matrix.\n",
    "    - V: A numpy array of shape (D, M) containing the weights for the value matrix.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data of shape (N, Tdec, M)\n",
    "    - cache: Values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    N, Tdec, D = x.shape\n",
    "    M = K.shape[-1]\n",
    " \n",
    "    beta = x @ np.expand_dims(Q, axis=0) @ np.expand_dims(K.T, axis=0) @ y.transpose(0,2,1)\n",
    "    beta /= np.sqrt(M)\n",
    "    alpha, softmax_cache = softmax_forward(beta.reshape(N*Tdec, -1))   # tensor (N, Tdec, Tenc)\n",
    "    alpha = alpha.reshape(N, Tdec, Tenc)\n",
    "    out = alpha @ y @ np.expand_dims(V, axis=0)\n",
    "\n",
    "    cache = (softmax_cache, alpha, x, y, K, Q, V)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def batch_cross_attention_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative of the cross-attention output of shape (N, Tdec, M)\n",
    "    - cache: A tuple of values from the forward pass.\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x, of shape (N, Tenc, D).\n",
    "    - dy: Gradient with respect to y, of shape (N, Tdec, D).\n",
    "    - dK: Gradient with respect to K, of shape (D, M).\n",
    "    - dQ: Gradient with respect to Q, of shape (D, M).\n",
    "    - dV: Gradient with respect to V, of shape (D, M).\n",
    "    \"\"\"\n",
    "    softmax_cache, alpha, x, y, K, Q, V = cache\n",
    "    N, Tdec, M = dout.shape\n",
    "\n",
    "    dV = np.sum((alpha @ y).transpose(0,2,1) @ dout, axis=0)\n",
    "    dalpha = dout @ (y @ np.expand_dims(V, axis=0)).transpose(0,2,1)\n",
    "    dbeta = softmax_backward(dalpha.reshape(N*Tdec,-1), softmax_cache) / np.sqrt(M)\n",
    "    dbeta = dbeta.reshape(N, Tdec, Tenc)\n",
    "    dQ = np.sum(x.transpose(0,2,1) @ dbeta @ y @ np.expand_dims(K, axis=0), axis=0)\n",
    "    dK = np.sum(y.transpose(0,2,1) @ dbeta.transpose(0,2,1) @ x @ np.expand_dims(Q, axis=0), axis=0)\n",
    "    dy = alpha.transpose(0,2,1) @ dout @ np.expand_dims(V.T, axis=0)\n",
    "    dy += dbeta.transpose(0,2,1) @ x @ np.expand_dims(Q @ K.T, axis=0)\n",
    "    dx = dbeta @ y @ np.expand_dims(K @ Q.T, axis=0)\n",
    "\n",
    "    return dx, dy, dK, dQ, dV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e79e3f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (2, 2, 4)\n",
      "z shape: (2, 4, 3)\n",
      "z:\n",
      " [[[-1.699  0.752  0.58 ]\n",
      "  [-2.284  0.682  0.421]\n",
      "  [-1.566  0.768  0.616]\n",
      "  [-2.338  0.676  0.407]]\n",
      "\n",
      " [[-1.699  0.752  0.58 ]\n",
      "  [-2.284  0.682  0.421]\n",
      "  [-1.566  0.768  0.616]\n",
      "  [-2.338  0.676  0.407]]]\n"
     ]
    }
   ],
   "source": [
    "N = 2\n",
    "y = np.stack([y] * N)\n",
    "x = np.stack([x] * N)\n",
    "z, cache = batch_cross_attention_forward(x, y, K[0], Q[0], V[0])\n",
    "\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"z shape:\", z.shape)\n",
    "print(\"z:\\n\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b47f3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x max relative error: 5.405021e-11\n",
      "y max relative error: 3.270507e-10\n",
      "K max relative error: 1.397881e-09\n",
      "Q max relative error: 4.778638e-10\n",
      "V max relative error: 3.774175e-11\n"
     ]
    }
   ],
   "source": [
    "l, dz = loss(z)\n",
    "dx, dy, dK, dQ, dV = batch_cross_attention_backward(dz, cache)\n",
    "params = {\"x\":x, \"y\":y, \"K\":K[0], \"Q\":Q[0], \"V\":V[0]}\n",
    "grads = {\"x\":dx, \"y\":dy, \"K\":dK, \"Q\":dQ, \"V\":dV}\n",
    "\n",
    "# These should all be less than 1e-8 or so.\n",
    "f = lambda a: loss(batch_cross_attention_forward(x, y, K[0], Q[0], V[0])[0])[0]\n",
    "for _name in grads:\n",
    "    grad_numeric = eval_numerical_gradient(f, params[_name], verbose=False)\n",
    "    print(\"%s max relative error: %e\" % (_name, rel_error(grad_numeric, grads[_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544126e3",
   "metadata": {},
   "source": [
    "Just as with self-attention, we again want to \"look\" at multiple places at once, and construct value vectors differently. Thus, we define multiple cross-attention heads using multiple $K$, $Q$, $V$ matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d5c0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_cross_attention_naive(x, y, K, Q, V, W):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (N, Tdec, D), giving decoder inputs.\n",
    "    - y: A numpy array of shape (N, Tenc, D), giving encoder outputs.\n",
    "    - K: A numpy array of shape (h, D, M) containing the weights for the key matrix.\n",
    "    - Q: A numpy array of shape (h, D, M) containing the weights for the query matrix.\n",
    "    - V: A numpy array of shape (h, D, M) containing the weights for the value matrix.\n",
    "    - W: A numpy array of shape (hM, D) contaning the weights for the transformation matrix.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data of shape (N, T, D)\n",
    "    - cache: Values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    N, Tdec, D = x.shape\n",
    "    h, _, M = K.shape\n",
    "    heads_out = []\n",
    "    \n",
    "    for i in range(h):\n",
    "        out, _ = batch_cross_attention_forward(x, y, K[i], Q[i], V[i]) # tensor of shape (N, T, M)\n",
    "        heads_out.append(out)\n",
    "    heads_out = np.dstack(heads_out)  # tensor of shape (N, T, hM)\n",
    "    print(\"heads out:\\n\", heads_out)\n",
    "    return heads_out @ np.expand_dims(W, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "033fa35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (2, 4, 4)\n",
      "\n",
      "Note that the output before transfromation is equal to\n",
      "the ouput from a single head concatenaded 3 times\n",
      "heads out:\n",
      " [[[-1.699  0.752  0.58  -1.699  0.752  0.58  -1.699  0.752  0.58 ]\n",
      "  [-2.284  0.682  0.421 -2.284  0.682  0.421 -2.284  0.682  0.421]\n",
      "  [-1.566  0.768  0.616 -1.566  0.768  0.616 -1.566  0.768  0.616]\n",
      "  [-2.338  0.676  0.407 -2.338  0.676  0.407 -2.338  0.676  0.407]]\n",
      "\n",
      " [[-1.699  0.752  0.58  -1.699  0.752  0.58  -1.699  0.752  0.58 ]\n",
      "  [-2.284  0.682  0.421 -2.284  0.682  0.421 -2.284  0.682  0.421]\n",
      "  [-1.566  0.768  0.616 -1.566  0.768  0.616 -1.566  0.768  0.616]\n",
      "  [-2.338  0.676  0.407 -2.338  0.676  0.407 -2.338  0.676  0.407]]]\n",
      "\n",
      "Output after transformation:\n",
      "z shape: (2, 4, 4)\n",
      "z:\n",
      " [[[ 3.324  3.524 -2.345 -6.67 ]\n",
      "  [ 4.444  4.265 -2.407 -8.694]\n",
      "  [ 3.069  3.355 -2.33  -6.208]\n",
      "  [ 4.547  4.333 -2.413 -8.881]]\n",
      "\n",
      " [[ 3.324  3.524 -2.345 -6.67 ]\n",
      "  [ 4.444  4.265 -2.407 -8.694]\n",
      "  [ 3.069  3.355 -2.33  -6.208]\n",
      "  [ 4.547  4.333 -2.413 -8.881]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"x shape:\", x.shape)\n",
    "\n",
    "print(\"\"\"\\nNote that the output before transfromation is equal to\n",
    "the ouput from a single head concatenaded {:d} times\"\"\".format(h))\n",
    "\n",
    "z = multihead_cross_attention_naive(x, y, K, Q, V, W)\n",
    "print(\"\\nOutput after transformation:\")\n",
    "print(\"z shape:\", z.shape)\n",
    "print(\"z:\\n\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d831da",
   "metadata": {},
   "source": [
    "And again, to make the multi-head cross-attention layer work using only matrix algebra we need to note that the gradient of the loss $\\ell$ with respect to the input tensors will be equal to the sum of the gradients from each attention head.\n",
    "\n",
    "$$ \\frac{d\\ell}{dx} = \\sum_{i=0}^{h-1} \\frac{d\\ell}{dx^{i}} $$\n",
    "$$ \\frac{d\\ell}{dy} = \\sum_{i=0}^{h-1} \\frac{d\\ell}{dy^{i}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfcd83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_crossattention_forward(x, y, K, Q, V, W):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (N, Tdec, D), giving decoder inputs.\n",
    "    - y: A numpy array of shape (N, Tenc, D), giving encoder outputs.\n",
    "    - K: A numpy array of shape (h, D, M) containing the weights for the key matrix.\n",
    "    - Q: A numpy array of shape (h, D, M) containing the weights for the query matrix.\n",
    "    - V: A numpy array of shape (h, D, M) containing the weights for the value matrix.\n",
    "    - W: A numpy array of shape (hM, D) contaning the weights for the transformation matrix.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data of shape (N, Tdec, D)\n",
    "    - cache: Values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    N, Tdec, D = x.shape\n",
    "    h, _, M = K.shape\n",
    "\n",
    "    beta = np.expand_dims(x, axis=1) @ np.expand_dims(Q, axis=0) \\\n",
    "         @ np.expand_dims(K.transpose(0,2,1), axis=0) @ np.expand_dims(y.transpose(0,2,1), axis=1)\n",
    "    beta /= np.sqrt(M)\n",
    "    alpha, softmax_cache = softmax_forward(beta.reshape(N*h*Tdec, -1))\n",
    "    alpha = alpha.reshape(N, h, Tdec, -1)\n",
    "    heads_out = alpha @ np.expand_dims(y, axis=1) @ np.expand_dims(V, axis=0)\n",
    "    heads_out = heads_out.transpose(0, 2, 1, 3).reshape(N, Tdec, h*M)\n",
    "    out, affine_cache = affine_forward(heads_out.reshape(N*Tdec, -1), W)\n",
    "    out = out.reshape(N, Tdec, D)\n",
    "\n",
    "    cache = (softmax_cache, affine_cache, alpha, x, y, K, Q, V, W)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def multihead_crossattention_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative of the multi-head cross-attention output of shape (N, Tdec, D)\n",
    "    - cache: A tuple of values from the forward pass.\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x, of shape (N, Tdec, D).\n",
    "    - dy: Gradient with respect to y, of shape (N, Tenc, D).\n",
    "    - dK: Gradient with respect to K, of shape (h, D, M).\n",
    "    - dQ: Gradient with respect to Q, of shape (h, D, M).\n",
    "    - dV: Gradient with respect to V, of shape (h, D, M).\n",
    "    \"\"\"\n",
    "    softmax_cache, affine_cache, alpha, x, y, K, Q, V, W = cache\n",
    "    N, Tdec, D = dout.shape\n",
    "    h, _, M = K.shape\n",
    "\n",
    "    dheads_out, dW, _ = affine_backward(dout.reshape(N*Tdec, -1), affine_cache)\n",
    "    dheads_out = dheads_out.reshape(N, Tdec, h, M).transpose(0, 2, 1, 3)\n",
    "\n",
    "    dV = np.sum((alpha @ np.expand_dims(y, axis=1)).transpose(0,1,3,2) @ dheads_out, axis=0)\n",
    "    dalpha = dheads_out @ (np.expand_dims(y, axis=1) @ np.expand_dims(V, axis=0)).transpose(0,1,3,2)\n",
    "    dbeta = softmax_backward(dalpha.reshape(N*h*Tdec,-1), softmax_cache) / np.sqrt(M)\n",
    "    dbeta = dbeta.reshape(N, h, Tdec, -1)\n",
    "    dQ = np.sum(np.expand_dims(x.transpose(0,2,1), axis=1) @ dbeta @ np.expand_dims(y, axis=1) \\\n",
    "        @ np.expand_dims(K, axis=0), axis=0)\n",
    "    dK = np.sum(np.expand_dims(y.transpose(0,2,1), axis=1) @ dbeta.transpose(0,1,3,2) \\\n",
    "        @ np.expand_dims(x, axis=1) @ np.expand_dims(Q, axis=0), axis=0)\n",
    "    dy = alpha.transpose(0,1,3,2) @ dheads_out @ np.expand_dims(V.transpose(0,2,1), axis=0)\n",
    "    dy += dbeta.transpose(0,1,3,2) @ np.expand_dims(x, axis=1) @ np.expand_dims(Q @ K.transpose(0,2,1), axis=0)\n",
    "    dx = dbeta @ np.expand_dims(y, axis=1) @ np.expand_dims(K @ Q.transpose(0,2,1), axis=0)\n",
    "    dx = np.sum(dx, axis=1)\n",
    "    dy = np.sum(dy, axis=1)\n",
    "\n",
    "    return dx, dy, dK, dQ, dV, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fbd8096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z shape: (2, 4, 4)\n",
      "z:\n",
      " [[[ 3.324  3.524 -2.345 -6.67 ]\n",
      "  [ 4.444  4.265 -2.407 -8.694]\n",
      "  [ 3.069  3.355 -2.33  -6.208]\n",
      "  [ 4.547  4.333 -2.413 -8.881]]\n",
      "\n",
      " [[ 3.324  3.524 -2.345 -6.67 ]\n",
      "  [ 4.444  4.265 -2.407 -8.694]\n",
      "  [ 3.069  3.355 -2.33  -6.208]\n",
      "  [ 4.547  4.333 -2.413 -8.881]]]\n"
     ]
    }
   ],
   "source": [
    "z, cache = multihead_crossattention_forward(x, y, K, Q, V, W)\n",
    "print(\"z shape:\", z.shape)\n",
    "print(\"z:\\n\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad4672b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x max relative error: 3.424784e-09\n",
      "y max relative error: 1.758840e-10\n",
      "K max relative error: 3.060167e-08\n",
      "Q max relative error: 1.574904e-08\n",
      "V max relative error: 1.308661e-09\n",
      "W max relative error: 5.105479e-11\n"
     ]
    }
   ],
   "source": [
    "l, dz = loss(z)\n",
    "dx, dy, dK, dQ, dV, dW = multihead_crossattention_backward(dz, cache)\n",
    "params = {\"x\":x, \"y\":y, \"K\":K, \"Q\":Q, \"V\":V, \"W\":W}\n",
    "grads = {\"x\":dx, \"y\":dy, \"K\":dK, \"Q\":dQ, \"V\":dV, \"W\":dW}\n",
    "\n",
    "# These should all be less than 1e-8 or so.\n",
    "f = lambda a: loss(multihead_crossattention_forward(x, y, K, Q, V, W)[0])[0]\n",
    "for _name in grads:\n",
    "    grad_numeric = eval_numerical_gradient(f, params[_name], verbose=False)\n",
    "    print(\"%s max relative error: %e\" % (_name, rel_error(grad_numeric, grads[_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746558c8",
   "metadata": {},
   "source": [
    "## THE TRANSFORMER ENCODER-DECODER\n",
    "\n",
    "The transformer encoder is composed of a self-attention layer and a feed-forward network.  \n",
    "The inputs are encoded with positional encodings and are then fed to the self-attention layer. The outputs of the self-attention layer are then fed to a feed-forward network. Every layer has a residual connection and is followed by a layer normalization.\n",
    "\n",
    "![Encoder](img/transformer_encoder.png \"Encoder\")\n",
    "\n",
    "\n",
    "The transformer decoder uses a masked self-attention layer, followed by a cross-attention layer, and a feed-forward network. The masked self-attention layer makes sure that the decoder does not \"look\" into the future by masking future positions. The cross-attention layer provides the functionality to attend to the input source sequence. Again, as with the encoder, every layer has a residual connection and is followed by a layer normalization.\n",
    "\n",
    "![Decoder](img/transformer_decoder.png \"Decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e12feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c084cc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example source:\n",
      " [[2 0 0 6]\n",
      " [2 4 9 3]]\n",
      "Example target:\n",
      " [[4 2 6 5 9 4 2 0]\n",
      " [3 5 3 6 5 1 2 8]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize toy example to check the implementation.\n",
    "np.random.seed(13)\n",
    "\n",
    "batch_size = 2\n",
    "src_seq_len = 4\n",
    "src_vocab_size = 10\n",
    "src_embed_dim = 4\n",
    "\n",
    "tgt_seq_len = 7\n",
    "tgt_vocab_size = 10\n",
    "tgt_embed_dim = 4\n",
    "\n",
    "hidden_dim = 5\n",
    "\n",
    "null_idx = 0\n",
    "start_idx = 1\n",
    "end_idx = 2\n",
    "\n",
    "src = np.random.randint(low=0, high=src_vocab_size, size=(batch_size, src_seq_len))\n",
    "tgt = np.random.randint(low=0, high=tgt_vocab_size, size=(batch_size, tgt_seq_len + 1))\n",
    "\n",
    "print(\"Example source:\\n\", src)\n",
    "print(\"Example target:\\n\", tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23371158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_0_dec relative error: 4.546203e-06\n",
      "K_0_enc relative error: 3.196035e-04\n",
      "K_1_dec relative error: 1.205586e-06\n",
      "K_1_enc relative error: 3.443817e-07\n",
      "K_dec_cross relative error: 1.267143e-06\n",
      "Q_0_dec relative error: 2.098227e-06\n",
      "Q_0_dec_cross relative error: 9.284055e-07\n",
      "Q_0_enc relative error: 1.371931e-06\n",
      "Q_1_dec relative error: 1.673130e-07\n",
      "Q_1_dec_cross relative error: 5.509773e-07\n",
      "Q_1_enc relative error: 3.215609e-07\n",
      "V_0_dec relative error: 3.681092e-08\n",
      "V_0_enc relative error: 1.455361e-07\n",
      "V_1_dec relative error: 4.208471e-07\n",
      "V_1_enc relative error: 5.975341e-07\n",
      "V_dec_cross relative error: 1.368287e-07\n",
      "W_0_dec relative error: 6.207619e-08\n",
      "W_0_dec_cross relative error: 3.587090e-08\n",
      "W_0_dec_ff relative error: 3.649067e-09\n",
      "W_0_enc relative error: 7.501286e-08\n",
      "W_0_enc_ff relative error: 1.740355e-08\n",
      "W_1_dec relative error: 4.006797e-07\n",
      "W_1_dec_cross relative error: 2.670082e-08\n",
      "W_1_dec_ff relative error: 1.062843e-08\n",
      "W_1_enc relative error: 7.265027e-08\n",
      "W_1_enc_ff relative error: 7.298267e-08\n",
      "W_embed_dec relative error: 4.657125e-09\n",
      "W_embed_enc relative error: 5.117841e-09\n",
      "W_out_dec relative error: 1.327928e-07\n",
      "b_0_dec_ff relative error: 2.540800e-09\n",
      "b_0_enc_ff relative error: 2.251442e-09\n",
      "b_1_dec_ff relative error: 1.614401e-09\n",
      "b_1_enc_ff relative error: 5.228272e-09\n",
      "b_out_dec relative error: 2.571474e-08\n",
      "beta1_0_dec relative error: 5.849056e-10\n",
      "beta1_0_enc relative error: 9.447538e-09\n",
      "beta1_1_dec relative error: 1.616202e-09\n",
      "beta1_1_enc relative error: 1.012624e-09\n",
      "beta2_0_dec relative error: 2.369565e-09\n",
      "beta2_0_enc relative error: 7.591155e-09\n",
      "beta2_1_dec relative error: 1.070432e-08\n",
      "beta2_1_enc relative error: 1.741269e-09\n",
      "beta3_0_dec relative error: 5.478007e-08\n",
      "beta3_1_dec relative error: 6.827105e-09\n",
      "gamma1_0_dec relative error: 3.095476e-09\n",
      "gamma1_0_enc relative error: 1.243417e-09\n",
      "gamma1_1_dec relative error: 3.059918e-07\n",
      "gamma1_1_enc relative error: 2.700600e-09\n",
      "gamma2_0_dec relative error: 1.104778e-09\n",
      "gamma2_0_enc relative error: 2.988065e-09\n",
      "gamma2_1_dec relative error: 4.921524e-09\n",
      "gamma2_1_enc relative error: 3.664334e-09\n",
      "gamma3_0_dec relative error: 5.348882e-08\n",
      "gamma3_1_dec relative error: 1.416432e-09\n"
     ]
    }
   ],
   "source": [
    "# Check the backward pass for the Transformer model.\n",
    "D = 4\n",
    "M = 3\n",
    "h = 5\n",
    "n_enc = 2\n",
    "n_dec = 2\n",
    "transformer_model = Transformer(D, M, h, n_enc, n_dec,\n",
    "                                src_vocab_size, src_embed_dim,\n",
    "                                tgt_vocab_size, tgt_embed_dim,\n",
    "                                null_idx, dtype=np.float64)\n",
    "\n",
    "\n",
    "loss, grads = transformer_model.loss(src, tgt)\n",
    "f = lambda _ : transformer_model.loss(src, tgt)[0]\n",
    "\n",
    "for param_name in sorted(grads):\n",
    "    param_grad_num = eval_numerical_gradient(f, transformer_model.params[param_name], verbose=False, h=1e-6)\n",
    "    print(\"%s relative error: %e\" % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
