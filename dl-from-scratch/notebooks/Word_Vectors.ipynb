{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# WORD VECTORS\n",
    "\n",
    "The first and arguably the most important common denominator across all NLP tasks is how to represent words as input to any model. We also need to have some notion of similarity and difference between words.\n",
    "\n",
    "Word vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, machine translation, etc. It is important to build some intuition as to their strengths and weaknesses.\n",
    "\n",
    "We want to encode word tokens each into some vector that represents a point in some sort of \"word\" space. Ideally, each dimension would encode some meaning that we transfer using speech. With word vectors, we can quite easily compute similarity of words using different measures such as Jaccard coefficient, Cosine similarity or Euclidean distance.\n",
    "\n",
    "There are different methods to find word embeddings (word vectors). The most simple word vector is the <b>one-hot vector</b>. It represents every word as an $ \\mathbb{R}^{|V| x 1} $ vector with all 0s and one 1 at the index if that word in the dictionary:\n",
    "\n",
    "$$ V['cat'] = 2 \\implies w^{cat} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} $$\n",
    "\n",
    "However, this representation does not give us directly any notion of similarity. All the word vectors are pairwise orthogonal. Using cosine similarity as an example, we would get:\n",
    "\n",
    "![Motel Hotel](img/word2vec_motel_hotel.png \"Motel Hotel\")\n",
    "\n",
    "\n",
    "The idea is to try to reduce the size of this vector space from $ \\mathbb{R}^{|V|} $ to something smaller and thus find a subspace that enocdes the relationship between words. The term \"embedding\" refers to the fact that we are encoding aspects of a word's meaning in a lower dimensional space.\n",
    "\n",
    "Many word vector implementations are driven by the idea that similar words (or even synonyms) will be used in similar contexts and, by examining these contexts, we can try to develop embeddings for our words. This idea is based on the famous quotation by John Rupert Firth <i>\"You shall know a word by the company it keeps.\"</i>\n",
    "\n",
    "![Word Context](img/word2vec_context.png \"Word Context\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "from src.word2vec import word2vec\n",
    "from src.solver import UnsupervisedSolver\n",
    "from src.layers import sigmoid, word_embedding_forward, negative_sampling_loss\n",
    "from src.utils.gradient_check import eval_numerical_gradient, rel_error\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# plot configuration\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Count-based Methods\n",
    "\n",
    "Many \"old school\" approaches to constructing word vectors relied on word counts. We loop over a massive dataset and accumulate word co-occurrence counts in some form of a matrix $ X $.\n",
    "\n",
    "### Word-Document Matrix\n",
    "\n",
    "We could make the conjecture that words that are related will often appear in the same documents and we use this fact to build a word-document matrix $ X $ in the following manner: loop over billions of documents and for each time a word <i>i</i> appears in document <i>j</i>, we add one to the entry $ X_{ij} $.\n",
    "\n",
    "Document 1: \"all that glitters is not gold\"  \n",
    "Document 2: \"all is well that ends well\"  \n",
    "\n",
    "|     *    | doc1 | doc2 |\n",
    "|----------|------|------|\n",
    "| all      |  1   |  1   |\n",
    "| that     |  1   |  1   |\n",
    "| glitters |  1   |  0   |\n",
    "| is       |  1   |  1   |\n",
    "| not      |  1   |  0   |\n",
    "| gold     |  1   |  0   |\n",
    "| well     |  0   |  2   |\n",
    "| ends     |  0   |  1   |\n",
    "\n",
    "Now, the rows of our matrix could be the vector embeddings of our words. However, the problem with this matrix is that it scales with the number of documents.\n",
    "\n",
    "### Window Co-occurrence Matrix\n",
    "\n",
    "Another approach we could try is to count the number of times each word appears inside a window of a particular size around the word of interest. Given a word in a document $ w_{i} $, we consider <i>the context window</i> of size $ n $ surrounding $ w_{i} $, i.e. the $ n $ preceeding and $ n $ subsequent words in that document ( $ w_{i-n}, ..., w_{i-1} $ and $ w_{i+1}, ..., w_{i+n} $ ).We build a co-occurrence matrix $ M $ in which $ M_{ij} $ is the number of times $ w_{j} $ occurs inside $ w_{i} $ 's window. To build the matrix we calculate this count for all the words and all the documents.\n",
    "\n",
    "Document 1: \"all that glitters is not gold\"  \n",
    "Document 2: \"all is well that ends well\"  \n",
    "\n",
    "\n",
    "|     *    | `<START>` | all | that | glitters | is   | not  | gold  | well | ends | `<END>` |\n",
    "|----------|-------|-----|------|----------|------|------|-------|------|------|-----|\n",
    "| `<START>`    | 0     | 2   | 0    | 0        | 0    | 0    | 0     | 0    | 0    | 0   |\n",
    "| all      | 2     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
    "| that     | 0     | 1   | 0    | 1        | 0    | 0    | 0     | 1    | 1    | 0   |\n",
    "| glitters | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
    "| is       | 0     | 1   | 0    | 1        | 0    | 1    | 0     | 1    | 0    | 0   |\n",
    "| not      | 0     | 0   | 0    | 0        | 1    | 0    | 1     | 0    | 0    | 0   |\n",
    "| gold     | 0     | 0   | 0    | 0        | 0    | 1    | 0     | 0    | 0    | 1   |\n",
    "| well     | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 1    | 1   |\n",
    "| ends     | 0     | 0   | 1    | 0        | 0    | 0    | 0     | 1    | 0    | 0   |\n",
    "| `<END>`      | 0     | 0   | 0    | 0        | 0    | 0    | 1     | 1    | 0    | 0   |\n",
    "\n",
    "<b>Note</b>: It is often the practice to add  `<START>` and `<END` tokens to represent the beginning and end of sentences, paragraphs or documents.\n",
    "\n",
    "Again, the rows of our matrix could be the vector embeddings of our words, however, the size of the vector space is still $ \\mathbb{R}^{|V|} $. To address this problem we can perform <i>dimensionality reduction</i> using <i>Singular Value Decomposition</i>. We decompose $ M = U \\Sigma V^{T} $ and select the top $ k $ principle components by truncating the matrix $ U $.\n",
    "\n",
    "In practice, it is challenging to apply full SVD to large corpora because of the memory needed to perform the decomposition. However, if you only want the top $ k $ vector components for a relatively small $ k $ then there are reasonably scalable techniques to compute those iteratively.\n",
    "\n",
    "<b>Note:</b> We could also perform SVD on the Word-Document matrix, however, the window-based assumption is much more reasonable. The Word-Document matrix could be regarded, in a sense, as a Window Co-occurrence matrix with variable window size equal to the length of the current document.\n",
    "\n",
    "\n",
    "## Iteration-based Methods\n",
    "\n",
    "Another approach is to try to create a model that will be able to learn one iteration at a time and eventually be able to encode the probability of a word given its context.\n",
    "\n",
    "The idea is to train a simple neural network with a single hidden layer to perform a cetain task, but then we're not actually going to use that neural network for the task we trained it on. Instead, the goal is to actually just learn the weights of the hidden layer. These weights are actually the \"word vectors\".\n",
    "\n",
    "A <b>language model</b> assigns probability to a sequence of tokens. A good language model will assign high probability to valid sentences and low probability to invalid sentences. Mathematically we can express this by:\n",
    "\n",
    "$$ \\displaystyle\n",
    "P(w_{1}, w_{2}, ..., w_{k}) = \\prod_{i=2}^{k} P(w_{i} | w_{i-1}, ..., w_{1}) $$\n",
    "\n",
    "for a sentence with $ k $ words.\n",
    "\n",
    "A <i>unary language model</i> assumes the word occurrences are completely independent:\n",
    "\n",
    "$$ \\displaystyle\n",
    "P(w_{1}, w_{2}, ..., w_{k}) = \\prod_{i=1}^{k} P(w_{i}) $$\n",
    "\n",
    "However, this model is not very accurate because we know that the next word is highly contingent upon the previous sequence of words. So we could try using an <i>n-gram language model</i> where we assume that each word occurence depends on the previous $ n-1 $ words.\n",
    "\n",
    "$$ \\displaystyle\n",
    "P(w_{1}, w_{2}, ..., w_{k}) = \\prod_{i=2}^{k} P(w_{i} | w_{i-1}, ..., w_{i-(n-1)}) $$\n",
    "\n",
    "The idea was further developed and two algorithms for learning word vectors were proposed in 2013 by Tomas Mikolov, Kai Chen, Greg Corrado and Jeff Dean in their paper:  \n",
    "<i>[1] \" Efficient Estimation of Word Representations in Vector Space\"</i>.\n",
    " * <b>Continuous bag-of-words (CBOW)</b>, which aims to predict a center word from the surrounding context.\n",
    " * <b>Skip-gram</b>, which aims to predict the distribution (probability) of context words from a center words."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CBOW\n",
    "\n",
    "#### The Fake Task\n",
    "The goal of the CBOW algorithm is: given context words to accurately learn to predict the center word.\n",
    "\n",
    "For given context words $ w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m} $ the probability of the center word is:\n",
    "\n",
    "$$ P(w_{t} |  w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}) $$\n",
    "\n",
    "The exact ordering of the words in the context is ignored, only the number of occurances of each term is material, thus the name <i>bag-of-words</i>.\n",
    "\n",
    "Given specific context words $ w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m} $, a question arrises how to calculate that probability $ P(w_{t} |  w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}) $. Assume we have <i>some</i> learned embeddings of the words (say $ u_{1}, u_{2}, ..., u_{|V|} $). For a given context around a center word $ w_{t} $, we take the average of the vectors of the context words:\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\upsilon_{t} = \\frac{u_{t-m} + ... + u_{t-1} + u_{t+1} + ... + u_{t+m}}{2m} $$\n",
    "\n",
    "Having obtained the vector $ \\upsilon_{t} $, we can use softmax classification to compute the probability for each word in the dictionary to be our center word.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} - \\space \\upsilon_{t} \\space - \\end{bmatrix}\n",
    "\\begin{bmatrix} | & | & \\cdots & | \\\\ \n",
    "                \\theta_{1} & \\theta_{2} & \\cdots & \\theta_{|V|} \\\\\n",
    "                | & | & \\cdots & |\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \\upsilon_{t} \\theta_{1} \\\\\n",
    "                \\upsilon_{t} \\theta_{2} \\\\\n",
    "                \\vdots \\\\\n",
    "                \\upsilon_{t} \\theta_{|V|}\n",
    "\\end{bmatrix}\n",
    "\\underset{softmax}{\\Rightarrow}\n",
    "\\begin{bmatrix} \n",
    "   \\displaystyle\\frac{exp(\\upsilon_{t} \\theta_{1})}{\\displaystyle \\sum_{i=1}^{|V|}exp(\\upsilon_{t} \\theta_{i})} \\\\\n",
    "   \\displaystyle\\frac{exp(\\upsilon_{t} \\theta_{2})}{\\displaystyle \\sum_{i=1}^{|V|}exp(\\upsilon_{t} \\theta_{i})} \\\\\n",
    "                \\vdots \\\\\n",
    "   \\displaystyle\\frac{exp(\\upsilon_{t} \\theta_{|V|})}{\\displaystyle \\sum_{i=1}^{|V|}exp(\\upsilon_{t} \\theta_{i})}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The weights of the softmax classifier are stored in a matrix $ W_{D x |V|} $.\n",
    "\n",
    "We now have:\n",
    "\n",
    "$$ \\displaystyle\n",
    "P(w_{t} | w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}) = \\frac{e^{\\upsilon_{t} \\theta_{t}}}{\\displaystyle\\sum_{i=1}^{|V|}e^{\\upsilon_{t} \\theta_{i}}} $$\n",
    "\n",
    "Let's say we have a vocabulary of $ |V| $ words, where each word is represented by a one-hot vector. If we store all the currently learned embeddings in a matrix $ U_{|V| x D} $ ($ D $ is the size of the embedding), then mapping a one-hot vector to its corresponding embedding can be seen as a vector-matrix multiplication. Multiplying a one-hot vector by a matrix effectively selects the matrix row whose index corresponds to the index of the 1 in the one-hot vector:\n",
    "\n",
    "$$ u_{t} =\n",
    "\\underbrace\n",
    "{\\begin{bmatrix} 0 & 0 & \\cdots & 0 & 1 & 0 & \\cdots & 0 \\end{bmatrix}}\n",
    "_{\\text{1 at position t}}\n",
    "\\begin{bmatrix} - \\space u_{1} \\space -  \\\\\n",
    "                - \\space u_{2} \\space -  \\\\\n",
    "                \\vdots \\\\\n",
    "                - \\space u_{t} \\space -  \\\\\n",
    "                \\vdots \\\\\n",
    "                - \\space u_{|V|} \\space -\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Calculating the sum of the embeddings of some words can be computed by stacking the one-hot vectors of these words one after another into a $ 1 x 2m|V| $ vector and stacking the embedding matrix in the same manner into a $ 2m|V| x D $ matrix:\n",
    "\n",
    "$$ \\upsilon_{t} =\n",
    "\\begin{bmatrix}\n",
    "0 & \\cdots & 1 & \\cdots & 0 & \\cdots & \\cdots & 0 & \\cdots & 1 & \\cdots & 0\n",
    "\\end{bmatrix}_{1 x 2m|V|}\n",
    "\\begin{bmatrix} - \\space u_{1} \\space -  \\\\ \n",
    "                \\vdots \\\\\n",
    "                - \\space u_{|V|} \\space - \\\\\n",
    "                - \\space u_{1} \\space -  \\\\ \n",
    "                \\vdots \\\\\n",
    "                - \\space u_{|V|} \\space - \\\\\n",
    "                \\vdots \\\\\n",
    "                \\vdots \\\\\n",
    "                - \\space u_{1} \\space -  \\\\ \n",
    "                \\vdots \\\\\n",
    "                - \\space u_{|V|} \\space -\n",
    "\\end{bmatrix}_{2m|V| x D} $$\n",
    "\n",
    "The entire process can be modelled as training a two-layer neural network. There is no activation function for the neurons in the hidden layer. The weights of the first layer are the embeddings we are trying to learn and the weights of the second layer are the weights for a softmax classifier.\n",
    "\n",
    "![CBOW](img/word2vec_cbow.png \"CBOW\")\n",
    "\n",
    "In practice, instead of multiplying one-hot vectors with the embedding matrix, we can perform simple table lookups to obtain embeddings of the contextual words and, after that, sum the vectors to arrive at the desired result.\n",
    "\n",
    "#### The objective function\n",
    "\n",
    "For each position in a text $ t = 1, ..., T $ we predict the center word given the context words within a window of size $ m $. Thus, the likelihood of the text is given by the product of the probabilities of the center words:\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\underbrace{\\text{Likelihood =} L(\\eta)}_{\\substack{\\eta \\space \\text{is all variables} \\\\ \\text{ to be optimized}}} = \\prod_{t=1}^{T} P(w_{t} |  w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}; \\eta) $$\n",
    "\n",
    "The objective function (the loss) is the average negative log of the likelihood:  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J(\\eta) = -\\frac{1}{T}logL(\\eta) = -\\frac{1}{T}\\sum_{t=1}^{T} logP(w_{t} |  w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}; \\eta) $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J(\\eta) = -\\frac{1}{T} \\sum_{t=1}^{T} log \\frac{e^{\\upsilon_{t}\\theta_{t}}}{\\displaystyle\\sum_{i=1}^{|v|} e^{\\upsilon_{t}\\theta_{i}}} $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J(\\eta) = \\frac{1}{T} \\sum_{t=1}^{T} (log \\sum_{i=1}^{|V|} e^{\\upsilon_{t}\\theta_{i}} -  \\upsilon_{t}\\theta_{t}) $$  \n",
    "\n",
    "\n",
    "The loss for each example is equal to the cross-entropy loss between the true distribution (the one-hot vector of the center word) and the predicted distribution.\n",
    "\n",
    "To perform one step in the optimization process, we need to find the gradients of the loss function with repsect to the parameters of the model $ \\frac{\\delta J}{\\delta U} $ and $ \\frac{\\delta J}{\\delta W} $. To calculate them we will use the chain rule:  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\delta J}{\\delta W} = \\frac{\\delta J}{\\delta scores} \\frac{\\delta scores}{\\delta W} $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\delta J}{\\delta U} = \\frac{\\delta J}{\\delta scores} \\frac{\\delta scores}{\\delta W} \\frac{\\delta W}{\\delta U} \\frac{\\delta U}{\\delta X} = \\frac{\\delta J}{\\delta W} \\frac{\\delta W}{\\delta U} \\frac{\\delta U}{\\delta X} $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\delta J}{\\delta scores} =\n",
    "\\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\delta J}{\\delta s_{11}} & \\displaystyle\\frac{\\delta J}{\\delta s_{12}} & \\cdots & \\displaystyle\\frac{\\delta J}{\\delta s_{1|V|}} \\\\\n",
    "\\displaystyle\\frac{\\delta J}{\\delta s_{21}} & \\displaystyle\\frac{\\delta J}{\\delta s_{22}} & \\cdots & \\displaystyle\\frac{\\delta J}{\\delta s_{2|V|}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\displaystyle\\frac{\\delta J}{\\delta s_{T1}} & \\displaystyle\\frac{\\delta J}{\\delta s_{T2}} & \\cdots & \\displaystyle\\frac{\\delta J}{\\delta s_{T|V|}}\n",
    "\\end{bmatrix} $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\delta J}{\\delta s_{ij}} = \\frac{\\delta J}{\\delta \\upsilon_{i}\\theta_{j}}\n",
    "= \\frac{1}{T} \\left( \\frac{e^{\\upsilon_{i}\\theta_{j}}}{\\displaystyle\\sum_{k=1}^{|V|} e^{\\upsilon_{i}\\theta_{k}}} - \\underbrace{1_{\\{y(i) = j\\}}}_{\\text{1 if the label of i is j}} \\right) $$  \n",
    "\n",
    "\n",
    "We will train the model by going through each word in the text and considering it as a center word. For every center word we consider a contextual window of a fixed size (for example <i>window_size</i>=2).\n",
    "\n",
    "![Word Context CBOW](img/word2vec_context_cbow.png \"Word Context CBOW\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Initialize random data.\n",
    "np.random.seed(seed=0)\n",
    "N = 5\n",
    "T = 4\n",
    "V = 10\n",
    "D = 8\n",
    "\n",
    "X = np.random.randint(low=0, high=V, size=(N, 2*T + 1))\n",
    "context = np.concatenate((X[:, : T], X[:, T + 1 : ]), axis=1)\n",
    "target = X[:, T].reshape(N, 1)\n",
    "\n",
    "print(\"context:\\n\", context)\n",
    "print(\"target:\\n\", target)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "context:\n",
      " [[5 0 3 3 9 3 5 2]\n",
      " [4 7 6 8 1 6 7 7]\n",
      " [8 1 5 9 9 4 3 0]\n",
      " [3 5 0 2 8 1 3 3]\n",
      " [3 7 0 1 9 0 4 7]]\n",
      "target:\n",
      " [[7]\n",
      " [8]\n",
      " [8]\n",
      " [3]\n",
      " [9]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Check the forward pass.\n",
    "np.random.seed(seed=0)\n",
    "CBOW = word2vec(vocab_size=V, embed_size=D,\n",
    "                model_type=\"cbow\", dtype=np.float64)\n",
    "\n",
    "word_vectors, _ = word_embedding_forward(context, CBOW.params[\"U\"])\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"word vectors:\\n\", word_vectors)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word vectors:\n",
      " [[[-0.332 -0.449 -0.54   0.617 -0.161 -0.139 -0.396  0.246]\n",
      "  [ 0.558  0.127  0.31   0.709  0.591 -0.309  0.3   -0.048]\n",
      "  [ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]\n",
      "  [ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]\n",
      "  [ 0.36  -0.39   0.127 -0.217 -0.275 -0.183 -0.099  0.018]\n",
      "  [ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]\n",
      "  [-0.332 -0.449 -0.54   0.617 -0.161 -0.139 -0.396  0.246]\n",
      "  [ 0.472 -0.065  0.099 -0.27  -0.807  0.207  0.273 -0.235]]\n",
      "\n",
      " [[-0.281 -0.626 -0.11   0.049  0.389  0.38  -0.122 -0.096]\n",
      "  [ 0.021  0.096 -0.201 -0.115 -0.213 -0.114 -0.257 -0.546]\n",
      "  [-0.51  -0.067 -0.283  0.122 -0.162 -0.373 -0.009  0.135]\n",
      "  [ 0.056 -0.127 -0.516  0.146 -0.287  0.016  0.231  0.041]\n",
      "  [-0.033  0.13   0.046  0.46   0.241  0.038  0.14   0.106]\n",
      "  [-0.51  -0.067 -0.283  0.122 -0.162 -0.373 -0.009  0.135]\n",
      "  [ 0.021  0.096 -0.201 -0.115 -0.213 -0.114 -0.257 -0.546]\n",
      "  [ 0.021  0.096 -0.201 -0.115 -0.213 -0.114 -0.257 -0.546]]\n",
      "\n",
      " [[ 0.056 -0.127 -0.516  0.146 -0.287  0.016  0.231  0.041]\n",
      "  [-0.033  0.13   0.046  0.46   0.241  0.038  0.14   0.106]\n",
      "  [-0.332 -0.449 -0.54   0.617 -0.161 -0.139 -0.396  0.246]\n",
      "  [ 0.36  -0.39   0.127 -0.217 -0.275 -0.183 -0.099  0.018]\n",
      "  [ 0.36  -0.39   0.127 -0.217 -0.275 -0.183 -0.099  0.018]\n",
      "  [-0.281 -0.626 -0.11   0.049  0.389  0.38  -0.122 -0.096]\n",
      "  [ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]\n",
      "  [ 0.558  0.127  0.31   0.709  0.591 -0.309  0.3   -0.048]]\n",
      "\n",
      " [[ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]\n",
      "  [-0.332 -0.449 -0.54   0.617 -0.161 -0.139 -0.396  0.246]\n",
      "  [ 0.558  0.127  0.31   0.709  0.591 -0.309  0.3   -0.048]\n",
      "  [ 0.472 -0.065  0.099 -0.27  -0.807  0.207  0.273 -0.235]\n",
      "  [ 0.056 -0.127 -0.516  0.146 -0.287  0.016  0.231  0.041]\n",
      "  [-0.033  0.13   0.046  0.46   0.241  0.038  0.14   0.106]\n",
      "  [ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]\n",
      "  [ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]]\n",
      "\n",
      " [[ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]\n",
      "  [ 0.021  0.096 -0.201 -0.115 -0.213 -0.114 -0.257 -0.546]\n",
      "  [ 0.558  0.127  0.31   0.709  0.591 -0.309  0.3   -0.048]\n",
      "  [-0.033  0.13   0.046  0.46   0.241  0.038  0.14   0.106]\n",
      "  [ 0.36  -0.39   0.127 -0.217 -0.275 -0.183 -0.099  0.018]\n",
      "  [ 0.558  0.127  0.31   0.709  0.591 -0.309  0.3   -0.048]\n",
      "  [-0.281 -0.626 -0.11   0.049  0.389  0.38  -0.122 -0.096]\n",
      "  [ 0.021  0.096 -0.201 -0.115 -0.213 -0.114 -0.257 -0.546]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Check the backward pass.\n",
    "loss, grads = CBOW.loss(X)\n",
    "\n",
    "f = lambda W: CBOW.loss(X)[0]\n",
    "for name in grads:\n",
    "    grad_numeric = eval_numerical_gradient(f, CBOW.params[name], verbose=False)\n",
    "    print('%s max relative error: %e' % (name, rel_error(grad_numeric, grads[name])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "W max relative error: 5.409319e-08\n",
      "U max relative error: 2.545553e-07\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Skip-Gram\n",
    "\n",
    "#### The Fake Task\n",
    "\n",
    "The goal of the Skip-Gram algorithm is: given a \"ceneter\" word $ w_{t} $ (the input word) to accurately learn the probability distribution of the contextual window surrounding $ w_{t} $. The output distribution is going to relate to how likely is to find each vocabulary word nearby our input word.\n",
    "\n",
    "For a given center word $ w_{t} $ the probability of the context words is:\n",
    "\n",
    "$$ P(w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m} | w_{t}) $$\n",
    "\n",
    "A key difference with <i>CBOW</i> is that <b>we make a Naive Bayes assumtion for conditional independence</b>. In other words, given the center word, all output words are independent.\n",
    "\n",
    "$$ \\displaystyle\n",
    "P(w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m} | w_{t}) \\space = \\prod_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} P(w_{t+j}|w_{t}) $$\n",
    "\n",
    "Given a specific word $ w_{k} $ we want to calculate the probability $ P(w_{k}|w_{t}) $, which is the probability that word $ w_{k} $ falls within the contextual window of $ w_{t} $. Again, assuming we have <i>some</i> learned embeddings of the words (say $ u_{1}, u_{2}, ..., u_{|V|} $), we can use softmax classification to compute that probability.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} - \\space u_{t} \\space - \\end{bmatrix}\n",
    "\\begin{bmatrix} | & | & \\cdots & | \\\\\n",
    "                \\theta_{1} & \\theta_{2} & \\cdots & \\theta_{|V|} \\\\\n",
    "                | & | & \\cdots & |\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} u_{t} \\theta_{1} \\\\\n",
    "                u_{t} \\theta_{2} \\\\\n",
    "                \\vdots \\\\\n",
    "                u_{t} \\theta_{|V|}\n",
    "\\end{bmatrix}\n",
    "\\underset{softmax}{\\Rightarrow}\n",
    "\\begin{bmatrix} \n",
    "   \\displaystyle\\frac{exp(u_{t} \\theta_{1})}{\\displaystyle \\sum_{i=1}^{|V|}exp(u_{t} \\theta_{i})} \\\\\n",
    "   \\displaystyle\\frac{exp(u_{t} \\theta_{2})}{\\displaystyle \\sum_{i=1}^{|V|}exp(u_{t} \\theta_{i})} \\\\\n",
    "                \\vdots \\\\\n",
    "   \\displaystyle\\frac{exp(u_{t} \\theta_{|V|})}{\\displaystyle \\sum_{i=1}^{|V|}exp(u_{t} \\theta_{i})}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We now have:\n",
    "\n",
    "$$ \\displaystyle\n",
    "P(w_{k}|w_{t}) = \\frac{e^{u_{t} \\theta_{k}}}{\\displaystyle \\sum_{i=1}^{|V|}e^{u_{t} \\theta_{i}}} $$  \n",
    "\n",
    "Again, If we store all the currently learned embeddings in a matrix $ U_{|V| x D} $, the entire process can be modelled as training a two-layer neural network, and again, instead of multiplying a one-hot vector with the embedding matrix, we perform a simple table lookup to obtain the desired result.\n",
    "\n",
    "![Skip-Gram](img/word2vec_skip-gram.png \"Skip-Gram\")\n",
    "\n",
    "\n",
    "#### The objective function\n",
    "\n",
    "For each position in the text $ t = 1, ..., T $ we predict the context words within a window of size $ m $. Thus, the likelihood of the text is given by the product of the probabilities of the context words for every center word:\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\underbrace{\\text{Likelihood =} L(\\eta)}_{\\substack{\\eta \\space \\text{is all variables} \\\\ \\text{ to be optimized}}} = \\prod_{t=1}^{T} \\prod_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} P(w_{t+j}\\space|w_{t}; \\eta) $$  \n",
    "\n",
    "The objective function (the loss) is the average negative log of the likelihood:\n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J(\\eta) = -\\frac{1}{T}logL(\\eta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} logP(w_{t+j}|w_{t}; \\eta) $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J(\\eta) = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{\\substack{-m \\leq k \\leq m \\\\ k \\neq 0}} log \\frac{e^{u_{t}\\theta_{t + k}}}{\\displaystyle\\sum_{l=1}^{|V|} e^{u_{t}\\theta_{l}}} $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J(\\eta) = \\frac{1}{T} \\sum_{t=1}^{T} \\left( 2m \\space log \\sum_{l=1}^{|V|} e^{u_{t}\\theta_{l}} \\space \\space - \\sum_{\\substack{-m \\leq k \\leq m \\\\ k \\neq 0}} u_{t}\\theta_{t + k} \\right) $$  \n",
    "\n",
    "The loss for each example is equal to the sum over the contextual words of the cross-entropy loss between the true distribution of a contextual word (the one-hot vector of the contextual word) and the predicted distribution.\n",
    "\n",
    "To calculate the gradients of the loss function with repsect to the parameters of the model $ \\displaystyle \\frac{\\delta J}{\\delta U} $ and $ \\displaystyle \\frac{\\delta J}{\\delta W} $ we will use the chain rule:  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\delta J}{\\delta W} = \\frac{\\delta J}{\\delta scores} \\frac{\\delta scores}{\\delta W} $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\delta J}{\\delta U} = \\frac{\\delta J}{\\delta scores} \\frac{\\delta scores}{\\delta W} \\frac{\\delta W}{\\delta U} \\frac{\\delta U}{\\delta X} = \\frac{\\delta J}{\\delta W} \\frac{\\delta W}{\\delta U} \\frac{\\delta U}{\\delta X} $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\delta J}{\\delta scores} =\n",
    "\\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\delta J}{\\delta s_{11}} & \\displaystyle\\frac{\\delta J}{\\delta s_{12}} & \\cdots & \\displaystyle\\frac{\\delta J}{\\delta s_{1|V|}} \\\\\n",
    "\\displaystyle\\frac{\\delta J}{\\delta s_{21}} & \\displaystyle\\frac{\\delta J}{\\delta s_{22}} & \\cdots & \\displaystyle\\frac{\\delta J}{\\delta s_{2|V|}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\displaystyle\\frac{\\delta J}{\\delta s_{T1}} & \\displaystyle\\frac{\\delta J}{\\delta s_{T2}} & \\cdots & \\displaystyle\\frac{\\delta J}{\\delta s_{T|V|}}\n",
    "\\end{bmatrix} $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\delta J}{\\delta s_{ij}} = \\frac{\\delta J}{\\delta u_{i}\\theta_{j}}\n",
    "= -\\frac{2m}{T} \\left( \\frac{e^{u_{i}\\theta_{j}}}{\\displaystyle \\sum_{l=1}^{|V|} e^{u_{i}\\theta_{l}}} - \\underbrace{1_{\\{y(i) = j\\}}}_{\\text{1 if the label of i is j}} \\right) $$  \n",
    "\n",
    "\n",
    "\n",
    "#### Training the model\n",
    "\n",
    "We will train the model by going through each word in the text and considering it as a center word. For every center word we consider a contextual window of a fixed size (for example <i>window_size=2</i>).\n",
    "\n",
    "![Word context Skip-Gram](img/word2vec_context_skip-gram.png \"Word Context Skip-Gram\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Check the forward pass.\n",
    "np.random.seed(seed=0)\n",
    "SkipGram = word2vec(vocab_size=V, embed_size=D,\n",
    "                    model_type=\"skipgram\", dtype=np.float64)\n",
    "\n",
    "word_vectors, _ = word_embedding_forward(target, SkipGram.params[\"U\"])\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"word vectors:\\n\", word_vectors)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word vectors:\n",
      " [[[ 0.021  0.096 -0.201 -0.115 -0.213 -0.114 -0.257 -0.546]]\n",
      "\n",
      " [[ 0.056 -0.127 -0.516  0.146 -0.287  0.016  0.231  0.041]]\n",
      "\n",
      " [[ 0.056 -0.127 -0.516  0.146 -0.287  0.016  0.231  0.041]]\n",
      "\n",
      " [[ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]]\n",
      "\n",
      " [[ 0.36  -0.39   0.127 -0.217 -0.275 -0.183 -0.099  0.018]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Check the backward pass.\n",
    "loss, grads = SkipGram.loss(X)\n",
    "\n",
    "f = lambda W: SkipGram.loss(X)[0]\n",
    "for name in grads:\n",
    "    grad_numeric = eval_numerical_gradient(f, SkipGram.params[name], verbose=False)\n",
    "    print('%s max relative error: %e' % (name, rel_error(grad_numeric, grads[name])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "W max relative error: 2.184301e-08\n",
      "U max relative error: 3.042708e-09\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NOTES\n",
    "\n",
    "<b>NOTE 1:</b> Obviously, with no non-linearity in the hidden layer the two matrix multiplications can be collapsed into a single matrix, and the entire neural network transforms into a linear classifier. However, as stated earlier, the goal is not to train a classifier, but to learn the weights of the hidden layer.\n",
    "\n",
    "<b>NOTE 2:</b> It is common to see the weights of the softmax classifier also reffered to as word embeddings. For CBOW $ u_{t} $ is the embedding of $ w_{t} $ when it is a center word, and $ \\theta_{t} $ is the embedding of $ w_{t} $ when it is a context word. And vice-versa for Skip-Gram. It is also common to use both vectors to arrive at the final embedding for the word $ w_{t} $ (via concatenation or averaging).\n",
    "\n",
    "<b>NOTE 3:</b> The neural network does not know anything about the offset of the output word relative to the input word. More distant words are usually less related to the current word than those close to it. To address this problem, the authors of the paper propose to give less weight to the distant words by sampling less from those words in our training examples. That is, the parameter $ m $ denotes the <i>maximal window size</i>. For each center word we select uniformly a number $ R $ in the range $ [1, m] $ and use $ R $ as the window size for this center word. This technique is referred to as <i>dynamic window size</i>.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "It deos not learn a different set of probabilities for the word before the input versus the word after. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training word vectors\n",
    "\n",
    "We will train word vectors using the Skip-Gram model on Wikipedia articles.  \n",
    "\n",
    "The dataset examined in this notebook can be downloaded from here:  \n",
    "http://mattmahoney.net/dc/text8.zip\n",
    "\n",
    "As stated earlier, we are interested in the model's parameters, not in its classification capabilities. However, if two different words appear in similar \"contexts\", then our model needs to output very similar results for these two words. And one way for the network to output similar results is if the word vectors for the context words are similar. Thus, our network is motivated to learn similar word vectors for similar words.  \n",
    "We will learn word vector representations for the most common words in the text and will replace rare words with `UNK` token. Vocabulary size greatly impacts the computation time needed for the model to learn the vector representations. Thus, we will constrain ourselves with a relatively small vocabulary.  \n",
    "The model uses L2 regularization of the weight matrices.  \n",
    "Parameter update is performed using Adam update rule."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Load the data.\n",
    "text8_dir = \"../datasets/text/\"\n",
    "with open(text8_dir + \"text8\", \"r\") as file:\n",
    "    text = file.read()\n",
    "    text = text.split()\n",
    "\n",
    "text = text[:1000000]\n",
    "\n",
    "print(\"Number of words in the dataset: \", len(text))\n",
    "print(\"\\nExample words: \", text[150:165])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of words in the dataset:  1000000\n",
      "\n",
      "Example words:  ['as', 'authoritarian', 'political', 'structures', 'and', 'coercive', 'economic', 'institutions', 'anarchists', 'advocate', 'social', 'relations', 'based', 'upon', 'voluntary']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Preprocess the data to create word-to-int and int-to-word mappings.\n",
    "# We will index the most common words and replace rare words with \"UNK\" token.\n",
    "# collections.Counter(list).most_common(N) returns a tuple of the N most common instances\n",
    "# contained in the list along with their count number\n",
    "vocab_size = 5000\n",
    "count = [(\"UNK\", 0)]\n",
    "count.extend(collections.Counter(text).most_common(vocab_size - 1))\n",
    "count = dict(count)\n",
    "count[\"UNK\"] = len(text) - sum(count.values())\n",
    "print(\"Most common words:\\n\", dict(itertools.islice(count.items(), 25)))\n",
    "\n",
    "word_to_idx = {w: i for i, w in enumerate(count)}\n",
    "idx_to_word = {i: w for i, w in enumerate(count)}\n",
    "print(\"Vocab:\\n\", dict(itertools.islice(word_to_idx.items(), 25)))   # slicing a dictionary with itertools.islice\n",
    "\n",
    "data = [word_to_idx.get(w, word_to_idx[\"UNK\"]) for w in text]\n",
    "\n",
    "print(\"\\nVocabulary size: %d\" % vocab_size)\n",
    "print(\"Example text data:\\n\", text[:57])\n",
    "print(\"After preprocessing:\\n\", data[:57])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Most common words:\n",
      " {'UNK': 144423, 'the': 62827, 'of': 36789, 'and': 25238, 'one': 24679, 'in': 22502, 'a': 18620, 'to': 18504, 'zero': 14349, 'nine': 14056, 'is': 11094, 'two': 10968, 'as': 7737, 'eight': 7708, 'three': 7049, 'was': 6892, 'by': 6796, 'five': 6647, 's': 6606, 'that': 6541, 'for': 6447, 'four': 6338, 'six': 6239, 'seven': 5914, 'with': 5672}\n",
      "Vocab:\n",
      " {'UNK': 0, 'the': 1, 'of': 2, 'and': 3, 'one': 4, 'in': 5, 'a': 6, 'to': 7, 'zero': 8, 'nine': 9, 'is': 10, 'two': 11, 'as': 12, 'eight': 13, 'three': 14, 'was': 15, 'by': 16, 'five': 17, 's': 18, 'that': 19, 'for': 20, 'four': 21, 'six': 22, 'seven': 23, 'with': 24}\n",
      "\n",
      "Vocabulary size: 5000\n",
      "Example text data:\n",
      " ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been']\n",
      "After preprocessing:\n",
      " [632, 3668, 12, 6, 173, 2, 4288, 47, 63, 150, 124, 870, 666, 0, 161, 1, 0, 2, 1, 111, 949, 3, 1, 0, 0, 2, 1, 138, 949, 4000, 1, 173, 10, 179, 63, 5, 6, 0, 212, 7, 1230, 105, 450, 19, 63, 2302, 365, 7, 3106, 1, 1049, 2, 367, 30, 41, 38, 52]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class text_dataset(object):\n",
    "    def __init__(self, data, window_size):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - data: List of integers representing the preprocessed data.\n",
    "          Every integer corresponds to a character.\n",
    "        - window_size: Integer giving the contextual window size.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.span = 2 * window_size + 1 # window - target - window\n",
    "\n",
    "    def train_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generate the next batch of examples from the data.\n",
    "\n",
    "        Returns:\n",
    "        - batch: A numpy array of integers of shape (batch_size, seq_length) giving\n",
    "          a batch of training examples.\n",
    "        \"\"\"\n",
    "        span = self.span\n",
    "        batch = np.ndarray((batch_size, span), dtype=np.int)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            cursor = np.random.randint(len(self.data)-span)\n",
    "            batch[idx] = self.data[cursor : cursor + span]\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def num_train(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - num_train: Integer, giving the number of training examples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Initialize the dataset.\n",
    "np.random.seed(seed=None)\n",
    "window_size = 2\n",
    "dataset = text_dataset(data, window_size)\n",
    "\n",
    "# Initialize the model\n",
    "batch_size = 128\n",
    "embed_size = 128\n",
    "SkipGram = word2vec(vocab_size=vocab_size, embed_size=embed_size,\n",
    "                    model_type=\"skipgram\",\n",
    "                    word_to_idx=word_to_idx,\n",
    "                    dtype=np.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Train the model.\n",
    "word2vec_solver = UnsupervisedSolver(SkipGram, dataset,\n",
    "                                     update_rule=\"adam\",\n",
    "                                     optim_config={\"learning_rate\": 1e-2},\n",
    "                                     lr_decay=0.5,\n",
    "                                     batch_size=batch_size,\n",
    "                                     clip_norm = 5.0,\n",
    "                                     num_epochs=2,\n",
    "                                     print_every=3500,\n",
    "                                     verbose=True)\n",
    "\n",
    "tic = time.time()\n",
    "word2vec_solver.train()\n",
    "toc = time.time()\n",
    "print(\"training took %.3f minutes\" % ((toc - tic) / 60))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of iterations per epoch: 7812\n",
      "(Iteration 1 / 15624) loss: 8.51641\n",
      "(Iteration 1 / 15624); Epoch(1 / 2); loss: 8.51641\n",
      "Sample:\n",
      " Nearest to friend: previously made literature profile signs serious website receives enemy features\n",
      "(Iteration 3501 / 15624) loss: 5.55426\n",
      "(Iteration 7001 / 15624) loss: 5.54042\n",
      "(Iteration 7813 / 15624); Epoch(2 / 2); loss: 5.35299\n",
      "Sample:\n",
      " Nearest to friend: daughter lady boy wife editor romantic colonel cousin replacement request\n",
      "(Iteration 10501 / 15624) loss: 5.32298\n",
      "(Iteration 14001 / 15624) loss: 5.47640\n",
      "(Iteration 15624 / 15624); Epoch(2 / 2); loss: 5.01387\n",
      "Sample:\n",
      " Nearest to friend: wife daughter hector husband son lady branden augusta sensei uncle\n",
      "training took 62.710 minutes\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Plot the loss function.\n",
    "plt.plot(word2vec_solver.loss_history)\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.gcf().set_size_inches(12, 4)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Print the nearest words\n",
    "common_words = np.array([word_to_idx[\"six\"], word_to_idx[\"eight\"], word_to_idx[\"three\"],\n",
    "                         word_to_idx[\"work\"], word_to_idx[\"friends\"], word_to_idx[\"king\"]])\n",
    "similar_words = SkipGram.sample(common_words, top_k=8)\n",
    "\n",
    "for line in similar_words:\n",
    "    print(line)\n",
    "\n",
    "# for idx in range(len(common_words)):\n",
    "#     word_id = common_words[idx]\n",
    "#     print(\"\\nNearest to %s: \" %(idx_to_word[word_id]), end=\"\")\n",
    "#     for ids in similar_words[idx]:\n",
    "#         print(idx_to_word[ids], end=\" \") "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nearest to six: eight seven four five three zero nine one\n",
      "Nearest to eight: seven six five nine three four zero one\n",
      "Nearest to three: four six eight five seven two one zero\n",
      "Nearest to work: attitude judgment appearance incident effort albums symbolic everyone\n",
      "Nearest to friends: wife mother fellow uncle absalom lover kurtz lectures\n",
      "Nearest to king: macedon constantine duke queen prince khan pope henry\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "U = SkipGram.params[\"U\"]\n",
    "norm_U = U / np.linalg.norm(U, axis=1, keepdims=True)\n",
    "\n",
    "king = norm_U[word_to_idx[\"king\"]]\n",
    "man = norm_U[word_to_idx[\"man\"]]\n",
    "woman = norm_U[word_to_idx[\"woman\"]]\n",
    "\n",
    "top_k = 8\n",
    "vect = king - man + woman\n",
    "args = np.argsort(norm_U.dot(vect))[::-1]\n",
    "nearest = args[1:top_k + 1]\n",
    "\n",
    "print(\"Nearest to 'king' - 'man' + 'woman': \", end=\"\")\n",
    "for ids in nearest:\n",
    "    print(idx_to_word[ids], end=\" \")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nearest to 'king' - 'man' + 'woman': woman emperor pope macedon prussia duke constantine prince "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Efficiency\n",
    "\n",
    "It should be noted that calculating the objective function is computationally huge. Any evaluation of the objective function would take $ O(|V|) $ time due to the summation over the entire vocabulary."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Generate batch.\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    batch = dataset.train_batch(batch_size)\n",
    "toc = time.time()\n",
    "print(\"Batch generation takes %.5f seconds\" % (toc - tic))\n",
    "\n",
    "# Compute the loss and gradients.\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    loss, grads = word2vec_solver.model.loss(batch)\n",
    "toc = time.time()\n",
    "print(\"Loss and grads computation takes %.5f seconds\" % (toc - tic))\n",
    "\n",
    "# Update the parameters.\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    for p, w in word2vec_solver.model.params.items():\n",
    "        dw = grads[p]\n",
    "        config = word2vec_solver.optim_configs[p]\n",
    "        next_w, next_config = word2vec_solver.update_rule(w, dw, config)\n",
    "toc = time.time()\n",
    "print(\"Parameter update takes %.5f seconds\" % (toc - tic))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batch generation takes 0.04376 seconds\n",
      "Loss and grads computation takes 1.97393 seconds\n",
      "Parameter update takes 0.31644 seconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "N, span = batch.shape\n",
    "window_size = (span - 1) // 2\n",
    "\n",
    "window = np.concatenate((batch[:, : window_size], batch[:, window_size + 1 : ]), axis=1)\n",
    "target = batch[:, window_size].reshape(N, 1)\n",
    "\n",
    "N, T = window.shape\n",
    "labels = window.reshape(N * T)\n",
    "\n",
    "scores, caches = SkipGram._forward(target)\n",
    "scores = scores[:, np.newaxis, :]\n",
    "scores = np.tile(scores, (1, T, 1))\n",
    "scores = scores.reshape(N * T, -1)\n",
    "N, C = scores.shape\n",
    "\n",
    "shifted_scores = scores - np.max(scores, axis=1, keepdims=True)\n",
    "\n",
    "# Compute the exponents.\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    exp_scores = np.exp(shifted_scores)\n",
    "toc = time.time()\n",
    "print(\"Computing the exp scores takes %.5f seconds\" % (toc - tic))\n",
    "\n",
    "# Computhe the loss\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    Y = np.log(np.sum(exp_scores, axis=1, keepdims=True))\n",
    "    Z = shifted_scores - Y\n",
    "    loss = - np.sum(Z[np.arange(N), labels]) / N\n",
    "toc = time.time()\n",
    "print(\"Computing the loss takes %.5f seconds\" % (toc - tic))\n",
    "\n",
    "# Compute the grads\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    dscores = np.exp(Z)\n",
    "toc = time.time()\n",
    "print(\"Computing the gradients step 1 takes %.5f seconds\" % (toc - tic))\n",
    "\n",
    "# Compute the grads\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    dscores[np.arange(N), labels] -= 1\n",
    "    dscores /= N\n",
    "toc = time.time()\n",
    "print(\"Computing the gradients step 2 takes %.5f seconds\" % (toc - tic))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Computing the exp scores takes 0.54444 seconds\n",
      "Computing the loss takes 0.06351 seconds\n",
      "Computing the gradients step 1 takes 0.48219 seconds\n",
      "Computing the gradients step 2 takes 0.02414 seconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Negative Sampling\n",
    "\n",
    "In 2014 Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeff Dean address the problem of effiiency in their paper:  \n",
    "<i>[2] \"Distributed Representations of Word Phrases and their Compositionality\"</i>.\n",
    "\n",
    "The proposed Negative-Sampling method is described in detail by Yoav Goldberg and Omer Levy in ther paper:  \n",
    "<i>[3] \"word2vec Explained: Deriving Mikolov et. al.'s Negative-Sampling Word-Embedding Method\"</i>.\n",
    "\n",
    "In the previous setting we considered the conditional probability $ P(w_{t} | w_{t-j}, ..., w_{t-1}, w_{t+1}, ..., w_{t+j}) = P(w|c) $ of a word given its context (or similarly $ P(c, w) $) from a given corpus of text, and the goal was to maximize to corpus probability:\n",
    "\n",
    "$$ \\displaystyle\n",
    "L(\\eta) = \\prod_{t=1}^{T} P(w | c; \\eta) \\space - \\space \\text{for the CBOW algorithm} $$  \n",
    "\n",
    "$$ \\displaystyle\n",
    "L(\\eta) = \\displaystyle \\prod_{t=1}^{T} P(c | w; \\eta) \\space - \\space \\text{for the Skip-Gram algorithm} $$\n",
    "\n",
    "To do this we model the conditional probability $ P(w | c; \\eta) $ using softmax:\n",
    "\n",
    "$$ \\displaystyle\n",
    "P(w | c; \\eta) = \\frac{e^{{u_{c}\\theta_{w}}}}{\\displaystyle\\sum_{v \\in V}e^{{u_{c}\\theta_{v}}}} $$\n",
    "\n",
    "However, computing this objective is very expensive due to the summation over the entire vocabulary. A more efficient way of deriving word embeddings would be to consider a different objective. Consider a pair $ (w, c) $ of word and context and denote by $ P(D = 1| w, c) $ the probability that this pair comes from the corpus data. We can model that probability using a sigmoid function:\n",
    "\n",
    "$$ \\displaystyle\n",
    "P(D = 1|w,c; \\eta) = \\frac{1}{1 + e^{ - u_{c} \\theta_{w}}} $$\n",
    "\n",
    "In order for the model to learn we have to present some $ (w,c) $ pairs which are not in the data. Now, the new objective function tries to maximize the probability of a pair being in the corpus data if it inedeed is, and maximize the probability of a pair not being in the corpus data if it indeed is not.  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "L(\\eta) = \\prod_{(w,c) \\in D} P (D = 1|w,c;\\eta) \\prod_{(w, c) \\notin D} (1 - P(D = 1|w,c;\\eta)) $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J(\\eta) = - log L(\\eta) = - \\left( \\sum_{(w,c) \\in D} log \\frac{1}{1 + e^{ - u_{c} \\theta_{w}}} + \\sum_{(w,c) \\notin D} log \\left( 1 - \\frac{1}{1 + e^{ - u_{c} \\theta_{w}}} \\right) \\right) $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J(\\eta) = - \\left( \\sum_{(w,c) \\in D} log \\frac{1}{1 + e^{ - u_{c} \\theta_{w}}} + \\sum_{(w,c) \\notin D} log \\frac{1}{1 + e^{u_{c} \\theta_{w}}} \\right) $$  \n",
    "\n",
    "\n",
    "For CBOW, our new objective function for observing the center word $ w_{t} $ given the context vector $ \\displaystyle \\hat{v} = \\frac{w_{t-j} + \\cdots + w_{t-1} + w_{t+1} + \\cdots + w_{t+j}}{2j} $ would be:  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J_{CBOW}(\\eta) = - log \\frac{1}{1 + e^{-\\theta_{t} \\hat{v}}} -  \\sum_{k=1}^{K} log \\frac{1}{1 + e^{\\theta_{k} \\hat{v}}} $$  \n",
    "\n",
    "\n",
    "Thus, the negative examples consist of our input context words and \"untrue\" center words.\n",
    "\n",
    "For Skip-Gram, our new objective function for observing the context words $ w_{t-j}, ..., w_{t-1}, w_{t+1}, ..., w_{t+j} $ given the center word $ w_{t} $ would be:  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J_{SkipGram}(\\eta) = - \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} log \\frac{1}{1 + e^{-w_{t} \\theta_{t+m}}} - \\sum_{k=1}^{K} log \\frac{1}{1 + e^{w_{t} \\theta_{k}}} $$\n",
    "\n",
    "Thus, the negative examples consist of our input center word and \"untrue\" context words.\n",
    "\n",
    "The words $ \\{w_{k}\\}_{k=1}^{K} $ are sampled from a noise distribution. The authors of [2] propose sampling from a uniform distribution raised to the power of $ 3/4 $. The authors also propose working with values of $ K = 5...20 $.\n",
    "\n",
    "\n",
    "#### NOTE:\n",
    "\n",
    "For computing the second term of the objective functions, we could choose both the center word $ w_{t} $ and the context words $ w_{t-j}, ..., w_{t-1}, w_{t+1}, ..., w_{t+j} $ of the negative examples from our noisy distribution.  \n",
    "However, keeping the input and randomly sampling only the \"untrue\" output has an appealing intuitive property. Firstly, for a given input we will only update the weights of the softmax classifier for the \"true\" output and for the randomly chosen \"untrue\" outputs. And secondly, for a given input we will only update the weights of the embeddings of that input by showing the model the \"true\" output and the randomly choosen \"untrue\" outputs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "np.random.seed(seed=0)\n",
    "N = 5\n",
    "V = 8\n",
    "K = 3\n",
    "T = 2\n",
    "\n",
    "s = np.array([np.arange(V) * 0.01] * N) * np.arange(1, N+1).reshape(N, 1)\n",
    "s = s[:, np.newaxis, :]\n",
    "s = np.tile(s, (1, T, 1))\n",
    "s = s.reshape(N*T, -1)\n",
    "\n",
    "y = np.random.randint(V, size=(N, T))\n",
    "z = np.random.randint(V, size=(N, K))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Check the forward pass.\n",
    "loss = 0\n",
    "for _n in range(N):\n",
    "    for _t in range(T):\n",
    "        loss -= np.log(sigmoid(s[_n, y[_n, _t]]))\n",
    "    for _k in range(K):\n",
    "        loss -= np.log(sigmoid(-s[_n, z[_n, _k]]))\n",
    "\n",
    "print(\"Loss = %.5f\" % loss)\n",
    "\n",
    "loss, _ = negative_sampling_loss(s, y, z)\n",
    "print(\"Loss = %.5f\" % loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss = 17.56620\n",
      "Loss = 17.56620\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Check the backward pass.\n",
    "loss, ds = negative_sampling_loss(s, y, z)\n",
    "\n",
    "f = lambda x: negative_sampling_loss(x, y, z)[0]\n",
    "ds_num = eval_numerical_gradient(f, s, verbose=False)\n",
    "print(\"max relative error: %e\" % (rel_error(ds, ds_num)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "max relative error: 3.865674e-10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Compare the efficiency.\n",
    "np.random.seed(seed=0)\n",
    "model = word2vec(vocab_size=vocab_size, embed_size=embed_size,\n",
    "                 model_type=\"skipgram\",\n",
    "                 word_to_idx=word_to_idx,\n",
    "                 dtype=np.float32)\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    loss, grads = model.loss(batch)\n",
    "toc = time.time()\n",
    "print(\"Loss and grads computation takes %.5f seconds.\" % (toc - tic))\n",
    "\n",
    "\n",
    "model = word2vec(vocab_size=vocab_size, embed_size=embed_size,\n",
    "                 model_type=\"skipgram\",\n",
    "                 negative_sampling=10,\n",
    "                 word_to_idx=word_to_idx,\n",
    "                 dtype=np.float32)\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    loss, grads = model.loss(batch)\n",
    "toc = time.time()\n",
    "print(\"Loss and grads computation takes %.5f seconds with negative sampling.\" % (toc - tic))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss and grads computation takes 2.14998 seconds.\n",
      "Loss and grads computation takes 0.76807 seconds with negative sampling.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training word vectors using Negative Sampling\n",
    "\n",
    "Again, we will train word vectors using the Skip-Gram model on Wikipedia articles. However, this time we will use the negative sampling loss to evaluate our model.    \n",
    "Again, we will learn word vector representations for the most common words in the text and will replace rare words with `UNK` token.  \n",
    "The model uses L2 regularization of the weight matrices.  \n",
    "Parameter update is performed using Adam update rule."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Initialize the model using negative sampling.\n",
    "np.random.seed(seed=None)\n",
    "batch_size = 128\n",
    "embed_size = 128\n",
    "num_negative_samples = 20\n",
    "\n",
    "SkipGram = word2vec(vocab_size=vocab_size, embed_size=embed_size,\n",
    "                    model_type=\"skipgram\",\n",
    "                    negative_sampling = num_negative_samples,   # use negative sampling\n",
    "                    word_to_idx=word_to_idx,\n",
    "                    dtype=np.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# Train the model.\n",
    "word2vec_solver = UnsupervisedSolver(SkipGram, dataset,\n",
    "                                     update_rule=\"adam\",\n",
    "                                     optim_config={\"learning_rate\": 1e-2},\n",
    "                                     lr_decay=0.5,\n",
    "                                     batch_size=batch_size,\n",
    "                                     clip_norm = 5.0,\n",
    "                                     num_epochs=2,\n",
    "                                     print_every=3500,\n",
    "                                     verbose=True)\n",
    "\n",
    "tic = time.time()\n",
    "word2vec_solver.train()\n",
    "toc = time.time()\n",
    "print(\"training took %.3f minutes\" % ((toc - tic) / 60))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of iterations per epoch: 7812\n",
      "(Iteration 1 / 15624) loss: 2130.28613\n",
      "(Iteration 1 / 15624); Epoch(1 / 2); loss: 2130.28613\n",
      "Sample:\n",
      " Nearest to friend: reality aidan pierre department railway members singles francisco poet adding\n",
      "(Iteration 3501 / 15624) loss: 766.36368\n",
      "(Iteration 7001 / 15624) loss: 772.96403\n",
      "(Iteration 7813 / 15624); Epoch(2 / 2); loss: 813.54270\n",
      "Sample:\n",
      " Nearest to friend: lipsius hop cooperation observer honorary management circular warrior muslim turks\n",
      "(Iteration 10501 / 15624) loss: 761.50716\n",
      "(Iteration 14001 / 15624) loss: 712.20203\n",
      "(Iteration 15624 / 15624); Epoch(2 / 2); loss: 731.53968\n",
      "Sample:\n",
      " Nearest to friend: temples rocky wild eggs slowly wheat seven summary steven pale\n",
      "training took 29.679 minutes\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Plot the loss function.\n",
    "plt.plot(word2vec_solver.loss_history)\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.gcf().set_size_inches(12, 4)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# Print the nearest words\n",
    "common_words = np.array([word_to_idx[\"six\"], word_to_idx[\"eight\"], word_to_idx[\"three\"],\n",
    "                         word_to_idx[\"work\"], word_to_idx[\"friends\"], word_to_idx[\"king\"]])\n",
    "similar_words = SkipGram.sample(common_words, top_k=8)\n",
    "\n",
    "for line in similar_words:\n",
    "    print(line)\n",
    "\n",
    "# for idx in range(len(common_words)):\n",
    "#     word_id = common_words[idx]\n",
    "#     print(\"\\nNearest to %s: \" %(idx_to_word[word_id]), end=\"\")\n",
    "#     for ids in similar_words[idx]:\n",
    "#         print(idx_to_word[ids], end=\" \") "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nearest to six: UNK a one the in was eggs bulk\n",
      "Nearest to eight: UNK the in was a by and is\n",
      "Nearest to three: the of UNK and to in zero temples\n",
      "Nearest to work: zero the nine one was and which this\n",
      "Nearest to friends: planning temples amide abydos worker diagnostic allah summary\n",
      "Nearest to king: temples rainfall archive circa reasoning antonio bulk dealt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "U = SkipGram.params[\"U\"]\n",
    "norm_U = U / np.linalg.norm(U, axis=1, keepdims=True)\n",
    "\n",
    "king = norm_U[word_to_idx[\"king\"]]\n",
    "man = norm_U[word_to_idx[\"man\"]]\n",
    "woman = norm_U[word_to_idx[\"woman\"]]\n",
    "\n",
    "top_k = 8\n",
    "vect = king - man + woman\n",
    "args = np.argsort(norm_U.dot(vect))[::-1]\n",
    "nearest = args[1:top_k + 1]\n",
    "\n",
    "print(\"Nearest to 'king' - 'man' + 'woman': \", end=\"\")\n",
    "for ids in nearest:\n",
    "    print(idx_to_word[ids], end=\" \")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nearest to 'king' - 'man' + 'woman': king algerian sister intervention h riots j outside "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}