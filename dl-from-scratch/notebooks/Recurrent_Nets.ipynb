{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECURRENT NEURAL NETWORKS\n",
    "\n",
    "A limitation of vanilla neural networks is that they are constrained: they accept a fixed-sized vector as input and produce a fixed-sized vector as output. Recurrent networks allow us to operate over sequences of vectors of arbitrary sizes.\n",
    "\n",
    "![RNN](img/recurrent_nets.png \"RNN\")\n",
    "\n",
    "At each timestep, there are two inputs to the hidden layer: the output of the previous layer $ h_{t-1} $, and the input at that timestep $ x_{t} $. The former is multiplied by a weight matrix $ W_{h} $, and the latter by a weight matrix $ W_{x} $. The result is then run through a non-linearity function to produce the output at the current timestep $ h_{t} $.\n",
    "\n",
    "$$ h^{(t)} = tanh(W_{h}h^{(t-1)} + W_{x}x^{(t)} + b) $$\n",
    "\n",
    "It is important to note that the same weights $ W_{x} $ and $ W_{h} $ are applied repeatedly at each timestep. Thus, the number of parameters the model has to learn is less, and most importantly, is independent of the length of the input sequence.  \n",
    "At each timestep we could optionally produce an output using the weight matrix $ W_{o} $.\n",
    "\n",
    "The initial hidden state $ h^{(0)} $ can be learned as a parameter of the network or it can be assumed to be 0. In some problems the initial hidden state is given as an input to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.recurrent_net import RecurrentNetwork\n",
    "from src.solver import UnsupervisedSolver\n",
    "from src.utils.gradient_check import eval_numerical_gradient, rel_error\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# plot configuration\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the network we must compute the output distribution $ \\hat{y}^{(t)} $ for every timestep $ t $. After that we compute the loss at every timestep as the cross-entropy between the predicted probability distribution $ \\hat{y^{(t)}} $ and the true distribution. Finally, we average this over the entire training batch to get the overall loss.\n",
    "\n",
    "![RNN Training](img/recurrent_net_training.png \"RNN Training\")\n",
    "\n",
    "One problem with recurrent neural networks is the vanishing gradient. During backpropagation, the contribution of gradient values gradually vanishes as they propagate to earlier timesteps. Thus, for deep networks, long-term effects are not accounted for. Due to vanishing gradients, we don't know whether there is no dependency between steps $ t $ and $ t+n $ in the data, or we just cannot capture the true dependency due to this issue.  \n",
    "It is too difficult for the RNN to learn to perserve information over many timesteps.\n",
    "\n",
    "![RNN Gradient](img/recurrent_net_gradient.png \"RNN Gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize toy example to check the implementation.\n",
    "np.random.seed(0)\n",
    "\n",
    "batch_size = 2\n",
    "timesteps = 3\n",
    "hidden_dim = 6\n",
    "input_dim = 20\n",
    "output_dim = 20\n",
    "\n",
    "X = np.random.randn(batch_size, timesteps, input_dim)\n",
    "y = np.random.randint(low=0, high=10, size=(batch_size, timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_out relative error: 4.655994e-04\n",
      "Wh_0 relative error: 1.131941e-01\n",
      "Wx_0 relative error: 8.751668e-05\n",
      "b_0 relative error: 4.989417e-06\n",
      "b_out relative error: 3.066667e-09\n"
     ]
    }
   ],
   "source": [
    "# Check the backward pass for a vanilla RNN cell.\n",
    "rnn_model = RecurrentNetwork(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                             n_layers=1, cell_type=\"rnn\", dtype=np.float64)\n",
    "\n",
    "loss, grads = rnn_model.loss(X, y)\n",
    "f = lambda _ : rnn_model.loss(X, y)[0]\n",
    "\n",
    "for param_name in sorted(grads):\n",
    "    param_grad_num = eval_numerical_gradient(f, rnn_model.params[param_name], verbose=False, h=1e-6)\n",
    "    print('%s relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory\n",
    "\n",
    "In 1997 Sepp Hochreiter and Jurgen Schmidhuber proposed a solution to the vanishing gradient problem in their paper:  \n",
    "<i>[1] \"Long Short-Term Memory\"</i>.\n",
    "\n",
    "In standard RNNs at every timestep a simple non-linearity is applied to the output of the previous layer $ h^{(t-1)} $ and the input at that timestep $ x^{(t)} $.\n",
    "\n",
    "In LSTMs at every timestep there is a hidden state $ h^{(t)} $ and a <b>cell state</b> $ c^{(t)} $. The idea of the cell state is to store long-term information. During the forward pass the LSTM can <i>erase</i>, <i>write</i> and <i>read</i> information from the cell state. The selection of which information is erased/written/read is controlled by three gates based on the previus inputs. Gates are a way to optionally let information through. They are composed of a sigmoid function that outputs numbers between zero and one, describing how much of each component should be let through.\n",
    "\n",
    "![RNN LSTM](img/recurrent_net_lstm.png \"RNN LSTM\")\n",
    "\n",
    "#### Forget Gate\n",
    "\n",
    "The desicion what information to throw away from the cell state is made by a sigmoid layer called the forget gate. At timestep $ t $ it \"looks\" at $ h^{(t-1)} $ and $ x^{(t)} $, and outputs a number between zero and one for each number in the cell state $ c^{(t-1)} $.\n",
    "\n",
    "$$ f_{t} = \\sigma \\space (W_{fh}h^{(t-1)} + W_{fx}x^{(t)} + b_{f}) $$\n",
    "\n",
    "#### Input Gate\n",
    "\n",
    "The next step is to decide what new information to store in the cell state. This operation has two parts:\n",
    " * a <i>tanh</i> layer creates a vector of new candidate values\n",
    " * a sigmoid layer called the input gate decides which values we will store\n",
    "\n",
    "$$ i_{t} = \\sigma \\space (W_{ih}h^{(t-1)} + W_{ix}x^{(t)} + b_{i}) $$\n",
    "$$ g_{t} = \\tanh \\space (W_{gh}h^{(t-1)} + W_{gx}x^{(t)} + b_{f}) $$\n",
    "\n",
    "The new cell state is calculated by multiplying the old state by $ f_{t} $ and adding the candidate values multiplied by $ i_{t} $:\n",
    "\n",
    "$$ c^{(t)} = f_{t} * c^{(t-1)} + i_{t} * g_{t} $$\n",
    "\n",
    "#### Output Gate\n",
    "\n",
    "Finally, the output gate decides what information we are going to output. The output is a filtered version of the current cell state and the output gate is again a sigmoid layer:\n",
    "\n",
    "$$ o_{t} = \\tanh \\space (W_{oh}h^{(t-1)} + W_{ox}x^{(t)} + b_{o}) $$\n",
    "$$ h^{t} = o_{t} * tanh(c^{t}) $$\n",
    "\n",
    "![RNN LSTM-Detailed](img/recurrent_net_lstm_detailed.png \"RNN LSTM Detailed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_out relative error: 4.572344e-05\n",
      "Wh_0 relative error: 1.601574e-01\n",
      "Wx_0 relative error: 1.390295e-01\n",
      "b_0 relative error: 1.608477e-01\n",
      "b_out relative error: 3.539295e-09\n"
     ]
    }
   ],
   "source": [
    "# Check the backward pass for an LSTM cell.\n",
    "lstm_model = RecurrentNetwork(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                              n_layers=1, cell_type=\"lstm\", dtype=np.float64)\n",
    "\n",
    "loss, grads = lstm_model.loss(X, y)\n",
    "f = lambda _ : lstm_model.loss(X, y)[0]\n",
    "\n",
    "for param_name in sorted(grads):\n",
    "    param_grad_num = eval_numerical_gradient(f, lstm_model.params[param_name], verbose=False, h=1e-6)\n",
    "    print('%s relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer RNNs\n",
    "\n",
    "One way to create more complex RNN models is to stack the networks in layers.  \n",
    "The first layer of the recurrent network reads the input and produces a sequence of hidden states. Now we could use this sequence of hidden states as an input to another recurrent network. We could stack layers in this manner to produce a multilayer RNN.\n",
    "\n",
    "![RNN Multilayer](img/recurrent_net_multilayer.png \"RNN Multilayer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_out relative error: 3.638050e-03\n",
      "Wh_0 relative error: 1.127214e-07\n",
      "Wh_1 relative error: 1.848132e-07\n",
      "Wh_2 relative error: 3.351967e-07\n",
      "Wx_0 relative error: 3.646608e-03\n",
      "Wx_1 relative error: 1.249184e-03\n",
      "Wx_2 relative error: 2.542450e-03\n",
      "b_0 relative error: 2.923031e-03\n",
      "b_1 relative error: 1.407397e-02\n",
      "b_2 relative error: 9.665872e-06\n",
      "b_out relative error: 7.677437e-10\n"
     ]
    }
   ],
   "source": [
    "# Check the backward pass for a multilayer RNN.\n",
    "multilayer_lstm_model = RecurrentNetwork(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                                         n_layers=3, cell_type=\"lstm\", dtype=np.float64)\n",
    "\n",
    "loss, grads = multilayer_lstm_model.loss(X, y)\n",
    "f = lambda _ : multilayer_lstm_model.loss(X, y)[0]\n",
    "\n",
    "for param_name in sorted(grads):\n",
    "    param_grad_num = eval_numerical_gradient(f, multilayer_lstm_model.params[param_name], verbose=False, h=1e-6)\n",
    "    print('%s relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Level Language Model\n",
    "\n",
    "We will train a character level language model using an LSTM recurrent network.  \n",
    "To train the model we wiil use a simple text file. We will encode every character into a one-hot vector encoding, and feed the network one character at a time. The size of our vocabulary will be the number of distinct characters in the dataset. At each time-step the output of the network will be a vector of dimension `vocab_size`, which we will interpret as the score the RNN assigns to each character comming next in the sequence.\n",
    "\n",
    "![RNN Chars](img/recurrent_net_chars.png \"RNN Chars\")\n",
    "\n",
    "During training, for subsequent inputs we will use the actual ground truth next character in the sequence and we stop generating characters once we generate the target length. Once we have our predicted target sequence, we compare it against our actual target sequence to calculate the loss.  \n",
    "During inference, every predicted character by the model we will feed as the next input and we keep generating characters until a certain amount of characters have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text: 13017 symbols\n",
      "Example text:\n",
      "\n",
      " THE CASK OF AMONTILLADO by EDGAR ALLAN POE\n",
      "\n",
      "THE thousand injuries of Fortunato I had\n",
      "borne as I best could, but when he ven-\n",
      "tured upon insult, I vowed revenge. You,\n",
      "who so well know the nature of my soul, will not\n",
      "suppose, however, that I gave utterance to a threat.\n",
      "AT LENGTH I would be avenged; this was a point de-\n",
      "finitively settle\n"
     ]
    }
   ],
   "source": [
    "# Load the data.\n",
    "filename = \"../datasets/text/the_cask_of_amontillado.txt\"\n",
    "with open(filename, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\"Length of the text: %d symbols\" % len(text))\n",
    "print(\"Example text:\\n\\n\", text[:336])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct characters: 59\n",
      "Vocab:\n",
      " {'e': 0, 'B': 1, ' ': 2, 'C': 3, 'M': 4, 'u': 5, '\\n': 6, ',': 7, 'F': 8, 'V': 9, 'I': 10, 'N': 11, 'n': 12, 'D': 13, 'c': 14, 'q': 15, 'y': 16, \"'\": 17, 'k': 18, 'z': 19, 'v': 20, '?': 21, 'p': 22, 'w': 23, '.': 24, 'K': 25, 'g': 26, 'l': 27, 'Y': 28, 'H': 29, 's': 30, '!': 31, 'W': 32, 'U': 33, 'E': 34, 'i': 35, ':': 36, 'A': 37, 'G': 38, '\"': 39, 'a': 40, ';': 41, 'j': 42, 'r': 43, 't': 44, 'R': 45, 'P': 46, 'S': 47, 'd': 48, 'b': 49, '-': 50, 'T': 51, 'o': 52, 'f': 53, 'O': 54, 'x': 55, 'L': 56, 'm': 57, 'h': 58}\n",
      "Example text after preprocessing:\n",
      " [51, 29, 34, 2, 3, 37, 47, 25, 2, 54, 8, 2, 37, 4, 54, 11, 51, 10, 56, 56, 37, 13, 54, 2, 49, 16, 2, 34, 13, 38]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data to create char-to-int and int-to-char mappings.\n",
    "chars = list(set(text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_idx = {ch: i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i,ch in enumerate(chars)}\n",
    "\n",
    "data = [char_to_idx[ch] for ch in text]\n",
    "\n",
    "print(\"Number of distinct characters: %d\" % vocab_size)\n",
    "print(\"Vocab:\\n\", char_to_idx)\n",
    "\n",
    "print(\"Example text after preprocessing:\\n\", data[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_dataset(object):\n",
    "    def __init__(self, data, seq_length):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - data: List of integers representing the preprocessed data.\n",
    "          Every integer corresponds to a character.\n",
    "        - seq_length: Integer giving the length of the sequence to be input\n",
    "          to the recurrent neural network.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def train_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generate the next batch of examples from the data.\n",
    "\n",
    "        Returns:\n",
    "        - batch: A numpy array of integers of shape (batch_size, seq_length) giving\n",
    "          a batch of training examples.\n",
    "        \"\"\"\n",
    "        seq_length = self.seq_length\n",
    "        batch = np.ndarray((batch_size, seq_length), dtype=np.int)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            cursor = np.random.randint(len(self.data)-seq_length)\n",
    "            batch[idx] = self.data[cursor : cursor + seq_length]\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def num_train(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - num_train: Integer, giving the number of training examples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLevelRNN(object):\n",
    "    def __init__(self, vocab_size, hidden_dim, char_to_idx,\n",
    "                 n_layers=1, reg=0.0, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - vocab_size: Integer giving the size of the vocabulary.\n",
    "        - hidden_dim: Integer giving the hidden size.\n",
    "        - n_layer: Integer giving the number of recurrent layers to use.\n",
    "        - dtype: Numpy datatype to use for computation.\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
    "\n",
    "        self.RNN = RecurrentNetwork(input_dim=vocab_size,\n",
    "                                    hidden_dim=hidden_dim,\n",
    "                                    output_dim=vocab_size,\n",
    "                                    n_layers=n_layers,\n",
    "                                    weight_scale=1,\n",
    "                                    reg=reg,\n",
    "                                    cell_type=\"lstm\",\n",
    "                                    dtype=dtype)\n",
    "\n",
    "        self.params = self.RNN.params\n",
    "\n",
    "    def one_hot_encoding(self, x, V):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: A numpy array of shape (N, T) giving a batch of integer indices.\n",
    "        \n",
    "        Returns:\n",
    "        - embed: A numpy array of shape (N, T vocab_size) giving one-hot encodings\n",
    "          of each element.\n",
    "        \"\"\"\n",
    "        x = x.astype(np.int)\n",
    "\n",
    "        if len(x.shape) == 1: # if T = 1\n",
    "            N = x.shape[0]\n",
    "            embed = np.zeros((N, V), dtype=np.int)\n",
    "            embed[np.arange(N), x] = 1\n",
    "        else:\n",
    "            N, T = x.shape\n",
    "            embed = np.zeros((N, T, V), dtype=np.int)\n",
    "            embed[np.array([np.arange(N)]*(T)).transpose(1, 0), [np.arange(T)]*N, x] = 1\n",
    "\n",
    "        return embed\n",
    "\n",
    "    def loss(self, X):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, T) of integers giving the input sequence\n",
    "          to the RNN. Each element is in the range 0 <= X[i, j] < V.\n",
    "  \n",
    "        Returns:\n",
    "        - loss: A scalar value giving the loss.\n",
    "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "          names to gradients of the loss with respect to those parameters.\n",
    "        \"\"\"\n",
    "        embed_x = self.one_hot_encoding(X, self.vocab_size)\n",
    "        embed_in = embed_x[:, : -1]\n",
    "        y = X[:, 1 :]\n",
    "        loss, grads = self.RNN.loss(embed_in, y)\n",
    "        \n",
    "        return loss, grads\n",
    "\n",
    "    def sample(self, start=None, max_length=100):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - start: A numpy array of shape (N, ) of integers giving input data.\n",
    "          If None, the sequence starts with \" \".\n",
    "        - max_length: Maximum length T of generated outputs.\n",
    "\n",
    "        Returns:\n",
    "        - sequence: Array of shape (N, max_length) giving sampled outputs.\n",
    "        \"\"\"\n",
    "        if start is None:\n",
    "            start = np.array([np.random.randint(self.vocab_size)])\n",
    "\n",
    "        N = start.shape[0]\n",
    "        start_embed = self.one_hot_encoding(start, self.vocab_size)\n",
    "        output = self.RNN.sample(start_embed, self.one_hot_encoding, max_length=max_length)\n",
    "        data = np.argmax(output, axis=2)\n",
    "        sequence = [[idx_to_char[idx] for idx in seq] for seq in data]\n",
    "        text = [''.join(seq) for seq in sequence]\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize the dataset.\n",
    "dataset = text_dataset(data, seq_length=seq_length)\n",
    "\n",
    "# Initialize the model.\n",
    "np.random.seed(seed=None)\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "reg = 1e-2\n",
    "\n",
    "char_level_model = CharLevelRNN(vocab_size=vocab_size,\n",
    "                                hidden_dim=hidden_dim,\n",
    "                                char_to_idx=char_to_idx,\n",
    "                                n_layers=n_layers,\n",
    "                                reg=reg,\n",
    "                                dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations per epoch: 203\n",
      "(Iteration 1 / 1015) loss: 98.05993\n",
      "(Iteration 1 / 1015); Epoch(1 / 5); loss: 98.05993\n",
      "Sample:\n",
      " vmee\n",
      "enee\n",
      "ee\n",
      "eeed\n",
      "oooolllloooolllloooollll!!ggtrr;;uttooll                                          \n",
      "(Iteration 204 / 1015); Epoch(2 / 5); loss: 66.12531\n",
      "Sample:\n",
      " \n",
      " he the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "(Iteration 407 / 1015); Epoch(3 / 5); loss: 57.76694\n",
      "Sample:\n",
      " e the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "(Iteration 501 / 1015) loss: 55.58795\n",
      "(Iteration 610 / 1015); Epoch(4 / 5); loss: 52.30736\n",
      "Sample:\n",
      " Ce the wall the wall the wall the wall the wall the wall the wall the wall the wall the wall the wal\n",
      "(Iteration 813 / 1015); Epoch(5 / 5); loss: 47.82888\n",
      "Sample:\n",
      " Le and the bece the bested the bested the bested the bested the bested the bested the bested the bes\n",
      "(Iteration 1001 / 1015) loss: 46.16495\n",
      "(Iteration 1015 / 1015); Epoch(5 / 5); loss: 46.37442\n",
      "Sample:\n",
      " Me and the cand the rece to the rece to the rece to the rece to the rece to the rece to the rece to \n",
      "training took 46.707 minutes\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "rnn_solver = UnsupervisedSolver(char_level_model, dataset,\n",
    "                                update_rule=\"adam\",\n",
    "                                optim_config={\"learning_rate\":1e-3},\n",
    "                                lr_decay=0.99,\n",
    "                                batch_size=batch_size,\n",
    "                                clip_norm = 50.0,\n",
    "                                num_epochs=5,\n",
    "                                print_every=500,\n",
    "                                verbose=True)\n",
    "\n",
    "tic = time.time()\n",
    "rnn_solver.train()\n",
    "toc = time.time()\n",
    "print(\"training took %.3f minutes\" % ((toc - tic) / 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
